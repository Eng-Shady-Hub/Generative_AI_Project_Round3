{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc5eab9f",
        "outputId": "ba0a368d-3ecc-4764-8e9b-790aa7139119"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06d3d702"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####    Genaral Functions   ####\n",
        "\n",
        "# 1- Function for showing the dataframe characteristics\n",
        "\n",
        "def data_details(df, n=5):\n",
        "\n",
        "     print(\"\\n\")\n",
        "     print(\"Shape:\")\n",
        "     print(df.shape)\n",
        "     print(\"\\n\")\n",
        "    \n",
        "     print(\"\\n The Head\")\n",
        "     display(df.head(n))  \n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n Info:\")\n",
        "     print(df.info(memory_usage=\"deep\"))\n",
        "     print(\"\\n\") \n",
        "\n",
        "     print(\"\\n The Null Values:\")\n",
        "     print(df.isnull().sum())\n",
        "     print(\"\\n\") \n",
        "\n",
        "     print(\"\\nSummary statistics (categorical):\")  # As our needed features are categorical \n",
        "     categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "     if len(categorical_cols) > 0:\n",
        "         display(df.describe(include=[object]))\n",
        "     else:\n",
        "         print(\"No categorical columns found.\")\n",
        "         print(\"\\n\")    \n",
        "\n",
        "# 2- Function for cleaning the DataFrame (Normalization)\n",
        "\n",
        "def clean_text(text, mode=\"input\" ,lowercase=True):\n",
        "                                 # input (features [ product_name , brand , category , subcatogry] or the target [description])\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "   \n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "     \n",
        "    text = re.sub(r\"<.*?>\", \" \", text) # Remove HTML tags\n",
        "    text = re.sub(r\"(https?://\\S+|www\\.\\S+|ftp://\\S+)\", \" \", text) # Remove URLs\n",
        "\n",
        "    if mode == \"description\":\n",
        "        text = re.sub(r\"[^a-z0-9\\s&-]\", \" \", text) # For descriptions: keep only letters, numbers, spaces, &, -\n",
        "    \n",
        "    else:  # mode == \"input features\"\n",
        "        \n",
        "        text = re.sub(r\"&\", \" & \", text) # Add spaces around &     \n",
        "        text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text)  # Split camelCase or PascalCase \n",
        "        text = re.sub(r\"(\\||/|>)\", \" > \", text) # Normalize separators (|, /, >) to \" > \" as some product names contains | symbole \n",
        "\n",
        "        if lowercase: #\n",
        "           text = re.sub(r\"[^a-z0-9\\s&'\\->]\", \" \", text)  # Keep allowed chars: a-z, 0-9, spaces, &, -, >,'(lowercase for category & sub_catogry)\n",
        "        else:\n",
        "           text = re.sub(r\"[^a-zA-Z0-9\\s&'\\->]\", \" \", text) # (uppercase for product_name & brand)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize multiple spaces to single space\n",
        "    text = re.sub(r\"( > )+\", \" > \", text) # Normalize multiple > in a row  \n",
        "    text = text.strip(\" >\") # Remove leading/trailing >\n",
        "\n",
        "    return text\n",
        "\n",
        "# 3- function to apply tokenization - lemmatization - stopword/punctuation removal\n",
        "# and keep the original casing for product names and brand\n",
        "\n",
        "def tokenize_lemmatize(text, product_name=None, brand=None):\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    preserve_tokens = set() # a set containing all the parts (tokens) of product name and brand.\n",
        "    if product_name:\n",
        "        preserve_tokens.update(product_name.split())\n",
        "    if brand:\n",
        "        preserve_tokens.update(brand.split())\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text in preserve_tokens:\n",
        "            tokens.append(token.text)  # keep original casing\n",
        "        elif not token.is_stop and token.is_alpha:\n",
        "            tokens.append(token.lemma_)  # lemmatize normal words\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "# 4- function to apply the clean_text and tokenize_lemmatize on our dataframe (Data preprocessiong step) \n",
        " \n",
        "def preprocess_dataset_clean_only(df, for_model=False): \n",
        "\n",
        "    clean_df = pd.DataFrame() # To return a new DataFrame with only cleaned columns\n",
        "\n",
        "    feature_cols = [\"product_name\", \"brand\"] # Clean feature columns\n",
        "    for col in feature_cols:\n",
        "        clean_df[f\"clean_{col}\"] = df[col].apply(lambda x: clean_text(x, mode=\"input\",lowercase=False )) # keep upercase \n",
        "\n",
        "    feature_cols = [ \"category\", \"sub_category\"]  # Clean feature columns\n",
        "    for col in feature_cols:\n",
        "        clean_df[f\"clean_{col}\"] = df[col].apply(lambda x: clean_text(x, mode=\"input\",lowercase=True )) # convert to lowercase \n",
        "    \n",
        "      \n",
        "    clean_df[\"clean_description\"] = df[\"description\"].apply(lambda x: clean_text(x, mode=\"description\", lowercase=True )) # Clean description\n",
        "\n",
        "#  Replace lowercase product/brand mentions with their original casing that is mentioned in description\n",
        "    for i, row in clean_df.iterrows():\n",
        "         desc = clean_df.at[i, \"clean_description\"]\n",
        "\n",
        "      # Handle product name parts\n",
        "         product_name = row[\"clean_product_name\"]\n",
        "         if product_name:\n",
        "             for token in product_name.split():\n",
        "                 pattern = r\"\\b\" + re.escape(token.lower()) + r\"\\b\"\n",
        "                 desc = re.sub(pattern, token, desc)\n",
        "\n",
        "     # Handle brand\n",
        "         brand = row[\"clean_brand\"]\n",
        "         if brand:\n",
        "             for token in brand.split():\n",
        "                 pattern = r\"\\b\" + re.escape(token.lower()) + r\"\\b\"\n",
        "                 desc = re.sub(pattern, token, desc)\n",
        "\n",
        "         clean_df.at[i, \"clean_description\"] = desc\n",
        "    \n",
        "    clean_df = clean_df.drop_duplicates(subset=[\"clean_description\"])  # Drop duplicates and empty descriptions\n",
        "    clean_df = clean_df[clean_df[\"clean_description\"] != \"\"].reset_index(drop=True)\n",
        "\n",
        "    # If preparing for model... add tokenization + lemmatization\n",
        "\n",
        "    if for_model:\n",
        "        clean_df[\"description_tokens\"] = clean_df.apply(\n",
        "            lambda row: tokenize_lemmatize(row[\"clean_description\"], \n",
        "                                           row[\"clean_product_name\"], \n",
        "                                           row[\"clean_brand\"]), axis=1)\n",
        "        clean_df = clean_df.drop(columns=[\"clean_description\"])\n",
        "        \n",
        "    return clean_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d19ab116"
      },
      "outputs": [],
      "source": [
        "### Download Data From Github\n",
        "\n",
        "zip_url = 'https://raw.githubusercontent.com/Eng-Shady-Hub/Generative_AI_Project_Round3/refs/heads/main/All_Datasets2.zip'\n",
        "response = requests.get(zip_url)\n",
        "response.raise_for_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataframes = {}\n",
        "\n",
        "# Open the ZIP file from memory\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "    # Collect only CSV files\n",
        "    csv_files = [f for f in z.namelist() if f.lower().endswith(\".csv\")]\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"No CSV files found in the ZIP.\")\n",
        "    else:\n",
        "        for i, file_name in enumerate(csv_files, start=1):\n",
        "            key = f\"df{i}\"\n",
        "            try:\n",
        "                with z.open(file_name) as f:\n",
        "                    # Try UTF-8 first; fallback to latin1 if decoding fails\n",
        "                    try:\n",
        "                        dataframes[key] = pd.read_csv(f, encoding='utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        f.seek(0)\n",
        "                        dataframes[key] = pd.read_csv(f, encoding='latin1')\n",
        "\n",
        "                    print(f'DataFrame \"{key}\" created from file: {file_name} (shape: {dataframes[key].shape})')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 1\n",
        "\n",
        "basket_data = dataframes[\"df1\"]\n",
        "data_details(basket_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# target = [description]\n",
        "# features = [product_name ,brand ,category, sub_category]\n",
        "\n",
        "basket_data = basket_data[[\"product\",\"brand\",\"category\" , \"sub_category\",\"description\"]]\n",
        "basket_data= basket_data.rename(columns={\"product\": \"product_name\"})\n",
        "\n",
        "print(basket_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# there are null values in product_name , brand & description columns\n",
        "\n",
        "basket_data=basket_data.dropna(subset=[\"description\",\"product_name\",\"brand\"])\n",
        "data_details(basket_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 2\n",
        "\n",
        "adidas_data = dataframes[\"df2\"]\n",
        "data_details(adidas_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unify columns names\n",
        "adidas_data = adidas_data[[\"Product Name\", \"Brand\", \"Description\"]].rename(columns={\"Product Name\": \"product_name\", \"Brand\": \"brand\" , \"Description\":\"description\"})\n",
        "adidas_data.info()\n",
        "\n",
        "# there are null values(only 3) in description column\n",
        "\n",
        "adidas_data=adidas_data.dropna(subset=[\"description\"])\n",
        "adidas_data =adidas_data[adidas_data['description'] != 'No description']\n",
        "adidas_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset has agood description to our model but not have category & sub_category columns\n",
        "# So we map category & sub_category columns with respect to the product_name column\n",
        "\n",
        "category_map = {\n",
        "    # cayrgory = footwear \n",
        "  \"shoe\": (\"footwear\", \"shoes\"),\"sneaker\": (\"footwear\", \"shoes\"),\"running\": (\"footwear\", \"running shoes\"),\"trainer\": (\"footwear\", \"trainers\"),\"cleat\": (\"footwear\", \"cleats\"),\n",
        "    \"slipper\": (\"footwear\", \"slippers\"),\"flip flop\": (\"footwear\", \"flip flops\"),\"jordan\": (\"footwear\", \"basketball shoes\"),\"retro\": (\"footwear\", \"shoes\"),\n",
        "    \"phantom\": (\"footwear\", \"cleats\"),\"venom\": (\"footwear\", \"cleats\"),\"mercurial\": (\"footwear\", \"soccer shoes\"),\"superfly\": (\"footwear\", \"soccer shoes\"),\n",
        "    \"tf\": (\"footwear\", \"turf soccer shoes\"),\"air max\": (\"footwear\", \"sneakers\"),\"p-6000\": (\"footwear\", \"running shoes\"),\"sandal\": (\"footwear\", \"sandals\"),\n",
        "    \"slide\": (\"footwear\", \"slides\"),\"adilette\": (\"footwear\", \"slides\"),\"flipflop\": (\"footwear\", \"flip flops\"),\"sb\": (\"footwear\", \"skate shoes\"),\"skate\": (\"footwear\", \"skate shoes\"),\n",
        "    \"chron\": (\"footwear\", \"skate shoes\"),\"kd\": (\"footwear\", \"basketball shoes\"),\"kyrie\": (\"footwear\", \"basketball shoes\"),\"iconclash\": (\"footwear\", \"running shoes\"),\n",
        "    \"daybreak\": (\"footwear\", \"sneakers\"),\"blazer\": (\"footwear\", \"sneakers\"),\"prelove\": (\"footwear\", \"sneakers\"),\"pegasus\": (\"footwear\", \"running shoes\"),\n",
        "    \"vaporfly\": (\"footwear\", \"running shoes\"),\"zoomx\": (\"footwear\", \"running shoes\"),\"slipon\": (\"footwear\", \"slip-ons\"),\"airforce\": (\"footwear\", \"sneakers\"),\n",
        "    \"airmax\": (\"footwear\", \"sneakers\"),\"metcon\": (\"footwear\", \"training shoes\"),\"court\": (\"footwear\", \"tennis shoes\"),\"pg\": (\"footwear\", \"basketball shoes\"),\n",
        "    \"m2k\": (\"footwear\", \"sneakers\"),\"winflo\": (\"footwear\", \"running shoes\"),\"vomero\": (\"footwear\", \"running shoes\"),\"vapormax\": (\"footwear\", \"lifestyle sneakers\"),\n",
        "    \"flip-flop\": (\"footwear\", \"flip flops\"),\"flip-flops\": (\"footwear\", \"flip flops\"),\"slip-on\": (\"footwear\", \"slip-ons\"), \"slip-ons\": (\"footwear\", \"slip-ons\"),\n",
        "    \"odyssey react\": (\"footwear\", \"running shoes\"),\"legend react\": (\"footwear\", \"running shoes\"),\"pre-love\": (\"footwear\", \"sneakers\"),\"air force\": (\"footwear\", \"sneakers\"),\n",
        "    \"drop-type\": (\"footwear\", \"running shoes\"),\"zoom rival fly\": (\"footwear\", \"running shoes\"),\"mx-720-818\": (\"footwear\", \"running shoes\"),\"tanjun\": (\"footwear\", \"running shoes\"),\n",
        "    \"superstar\": (\"footwear\", \"sneakers\"),\"slip on\": (\"footwear\", \"slip-ons\"),\"lebron soldier\": (\"footwear\", \"basketball shoes\"),\"react element\": (\"footwear\", \"running shoes\"),\n",
        "    \"free rn\": (\"footwear\", \"running shoes\"),\"zoom fly\": (\"footwear\", \"running shoes\"),\"zoom rise\": (\"footwear\", \"running shoes\"),\"tiempo legend\": (\"footwear\", \"soccer shoes\"),\n",
        "    \"flex rn\": (\"footwear\", \"running shoes\"),\"air zoom structure\": (\"footwear\", \"running shoes\"),\"sfb gen 2\": (\"footwear\", \"boots\"),\"air huarache\": (\"footwear\", \"sneakers\"),\n",
        "    \"wildhorse\": (\"footwear\", \"running shoes\"),\"benassi\": (\"footwear\", \"slides\"),\"terra kiger\": (\"footwear\", \"running shoes\"),\"classic cortez\": (\"footwear\", \"sneakers\"),\n",
        "    \"renew run\": (\"footwear\", \"running shoes\"),\"free tr\": (\"footwear\", \"training shoes\"),\"lebron\": (\"footwear\", \"basketball shoes\"),\"mowabb\": (\"footwear\", \"sneakers\"),\n",
        "    \"revolution\": (\"footwear\", \"running shoes\"),\"precision\": (\"footwear\", \"basketball shoes\"),\"shox\": (\"footwear\", \"running shoes\"),\"potential\": (\"footwear\", \"basketball shoes\"),\n",
        "    \"epic react\": (\"footwear\", \"running shoes\"), \"react city\": (\"footwear\", \"running shoes\"),\"kawa\": (\"footwear\", \"slides\"),\"joyride run\": (\"footwear\", \"running shoes\"),\n",
        "    \"joyride optik\": (\"footwear\", \"running shoes\"),\"flex contact\": (\"footwear\", \"running shoes\"),\"football\": (\"footwear\", \"Football Shoes\"),\"predator\": (\"footwear\", \"Football Shoes\"),\n",
        "    \"vandalised\": (\"footwear\", \"Casual Shoes\"),\"canyon\": (\"footwear\", \"Casual Shoes\"),\"react\": (\"footwear\", \"Running Shoes\"),\"acg\": (\"footwear\", \"Outdoor Shoes\"),\n",
        "    \"flex\": (\"footwear\", \"Training Shoes\"),\"signal\": (\"footwear\", \"Running Shoes\"),\"joyride\": (\"footwear\", \"Running Shoes\"),\"cortez\": (\"footwear\", \"Casual Shoes\"),\n",
        "    \"hawkins\": (\"footwear\", \"Casual Shoes\"),\"nemeziz\": (\"footwear\", \"Football Shoes\"),\"indoor\": (\"footwear\", \"Indoor Shoes\"),\"outdoor\": (\"footwear\", \"Outdoor Shoes\"),\n",
        "    \"trail\": (\"footwear\", \"Outdoor Shoes\"),\"superrep\": (\"footwear\", \"Training Shoes\"),\"zoom\": (\"footwear\", \"Running Shoes\"),\"tr\": (\"footwear\", \"Training Shoes\"),\n",
        "    \"renew\": (\"footwear\", \"Running Shoes\"),\"ghost\": (\"footwear\", \"Running Shoes\"),\"racer\": (\"footwear\", \"Running Shoes\"),\"alphadunk\": (\"footwear\", \"Basketball Shoes\"),\n",
        "    \"monarch\": (\"footwear\", \"Walking Shoes\"),\"af-1\": (\"footwear\", \"Casual Shoes\"),\"bella\": (\"footwear\", \"Casual Shoes\"), \"huarache\": (\"footwear\", \"Lifestyle Shoes\"),\n",
        "    \"solarsoft\": (\"footwear\", \"Training Shoes\"),\"exp-x14\": (\"footwear\", \"Running Shoes\"),\"fly.by\": (\"footwear\", \"Basketball Shoes\"),\"xarr\": (\"footwear\", \"Training Shoes\"),\n",
        "    \"skarn\": (\"footwear\", \"Casual Shoes\"),\"tailwind\": (\"footwear\", \"Running Shoes\"), \"air dsvm\": (\"footwear\", \"Running Shoes\"),\n",
        "    # category = accessories\n",
        "    \"sock\": (\"accessories\", \"socks\"), \"cap\": (\"accessories\", \"cap\"),\"hat\": (\"accessories\", \"cap\"),\"bag\": (\"accessories\", \"bag\"),\"backpack\": (\"accessories\", \"bag\"),\n",
        "    \"watch\": (\"accessories\", \"watch\")\n",
        "    }\n",
        "    \n",
        "def categorize_product(name):\n",
        "    name = str(name).lower()\n",
        "    for keyword, (cat, subcat) in category_map.items():\n",
        "        if keyword in name:\n",
        "            return cat, subcat\n",
        "    return \"Other\", \"Other\"  # fallback if no keyword found\n",
        "\n",
        "adidas_data[[\"category\", \"sub_category\"]] = adidas_data[\"product_name\"].apply(lambda x: pd.Series(categorize_product(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# arranging the columns to be the same in all datasets\n",
        "\n",
        "adidas_data = adidas_data[[\"product_name\", \"brand\",\"category\", \"sub_category\", \"description\"]]\n",
        "print(adidas_data.columns)\n",
        "data_details(adidas_data , n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 3\n",
        "\n",
        "amazon_data =dataframes[\"df3\"] \n",
        "data_details(amazon_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# there is no null values in the prefered dataset features\n",
        "#  Amazon dataset don't contain brand , we note the first name in the product_name is the brand\n",
        "# So creating a function to map the brand column with respect to product_name column\n",
        "\n",
        "def map_brand(name):\n",
        "    return name.split()[0]\n",
        "\n",
        "# Apply function\n",
        "amazon_data['brand'] =amazon_data['product_name'].apply(map_brand)\n",
        "\n",
        "#  Amazon dataset don't contain sub_category , we note the values in category colums are diveded by | \n",
        "# So creating it by map sub_category column with respect to category column by extracting the most specific level(last part)\n",
        "\n",
        "amazon_data['sub_category'] = amazon_data['category'].apply(lambda x: x.split('|')[-1])\n",
        "\n",
        "amazon_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# arranging the columns to be the same in all datasets\n",
        "\n",
        "amazon_data = amazon_data[[\"product_name\", \"brand\",\"category\", \"sub_category\", \"about_product\"]]\n",
        "amazon_data= amazon_data.rename(columns={\"about_product\": \"description\"})\n",
        "print(amazon_data.columns)\n",
        "data_details(amazon_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 4\n",
        " \n",
        "flipkart_data = dataframes[\"df4\"]\n",
        "data_details(flipkart_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# there are nulls in description an brand columns\n",
        "# clearing \"discription\" rows with missed values\n",
        " \n",
        "flipkart_data=flipkart_data.dropna(subset=[\"description\"])\n",
        "flipkart_data =flipkart_data[flipkart_data['description'] != 'No description']\n",
        "flipkart_data.isnull().sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# display the most common brand to fill the missing value \n",
        "\n",
        "most_common = flipkart_data['brand'].mode()[0]\n",
        "print(most_common)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filling the missed value of brand By common brand in our dataset \"REEB\"\n",
        "\n",
        "flipkart_data['brand'].fillna(\"REEB\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# arranging the columns to be the same in all datasets\n",
        "\n",
        "flipkart_data = flipkart_data[[\"title\", \"brand\",\"category\", \"sub_category\", \"description\"]]\n",
        "flipkart_data= flipkart_data.rename(columns={\"title\": \"product_name\"})\n",
        "print(flipkart_data.columns)\n",
        "flipkart_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 5\n",
        "\n",
        "adidas2_data =dataframes[\"df5\"] \n",
        "data_details(adidas2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adidas2 dataset not have null values\n",
        "#  Noting the breadcrumbs colums contains sub_category\n",
        "\n",
        "adidas2_data = adidas2_data[[\"name\", \"brand\",\"category\", \"breadcrumbs\", \"description\"]]\n",
        "adidas2_data= adidas2_data.rename(columns={\"name\": \"product_name\" , \"breadcrumbs\":\"sub_category\"})\n",
        "print(adidas2_data.columns)\n",
        "data_details(adidas2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 6\n",
        "\n",
        "elec_data = dataframes[\"df6\"]\n",
        "data_details(elec_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The elec_data dataSet is clear \n",
        "\n",
        "elec_data= elec_data.rename(columns={\"Product_name\": \"product_name\"})\n",
        "print(elec_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame 7\n",
        "\n",
        "Bigbasket2 = dataframes[\"df7\"]\n",
        "data_details(Bigbasket2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Bigbasket2=Bigbasket2[[\"SKU Name\",\"Brand\",\"Category\",\"Sub-Category\",\"About the Product\"]]\n",
        "data_details(Bigbasket2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename the column and clean the row with null or no description\n",
        "\n",
        "Bigbasket2=Bigbasket2.rename(columns={\"SKU Name\": \"product_name\" , \"Brand\":\"brand\",\"Category\":\"category\", \"Sub-Category\":\"sub_category\" ,\"About the Product\":\"description\"})\n",
        "Bigbasket2=Bigbasket2.dropna(subset=[\"description\"])\n",
        "Bigbasket2 =Bigbasket2[Bigbasket2['description'] != 'No description']\n",
        "Bigbasket2.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the brand column has only 3 null values\n",
        "\n",
        "Bigbasket2=Bigbasket2.dropna(subset=[\"brand\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_details(Bigbasket2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combining all datasets\n",
        "\n",
        "Final_data = pd.concat([basket_data, adidas_data, amazon_data ,adidas2_data , flipkart_data , elec_data  ,Bigbasket2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combining all datasets\n",
        "\n",
        "Final_data = pd.concat([basket_data, adidas_data, amazon_data ,adidas2_data , flipkart_data , elec_data  ,Bigbasket2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_details(Final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = os.path.expanduser(\"~/Documents/Final_data.csv\")\n",
        "Final_data.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset Saved to : \", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Just clean (ready for Fine-Tune Pretrained Model)\n",
        "clean_Final_data = preprocess_dataset_clean_only(Final_data, for_model=False)\n",
        "data_details(clean_Final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = os.path.expanduser(\"~/Documents/clean_Final_data.csv\")\n",
        "clean_Final_data.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset Saved to : \", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean + tokens (ready for model from scratch)\n",
        "\n",
        "clean_Final_data_model = preprocess_dataset_clean_only(Final_data, for_model=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_Final_data_model.head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "depi_projects",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
