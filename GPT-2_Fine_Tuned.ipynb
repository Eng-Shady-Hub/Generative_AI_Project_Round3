{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc5eab9f"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "06d3d702"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "05MNOTLO1Uk4"
      },
      "outputs": [],
      "source": [
        "######################################## General Functions #########################3\n",
        "\n",
        "# 1- Function for showing the dataframe characteristics\n",
        "\n",
        "def data_details(df, n=5):\n",
        "     print(\"\\n\")\n",
        "     print(\"Shape:\")\n",
        "     print(df.shape)\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n The Head\")\n",
        "     display(df.head(n))\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n Info:\")\n",
        "     print(df.info(memory_usage=\"deep\"))\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n The Null Values:\")\n",
        "     print(df.isnull().sum())\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\nSummary statistics (categorical):\")  # As our needed features are categorical\n",
        "     categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "     if len(categorical_cols) > 0:\n",
        "         display(df.describe(include=[object]))\n",
        "     else:\n",
        "         print(\"No categorical columns found.\")\n",
        "         print(\"\\n\")\n",
        "\n",
        "# 2- Function for cleaning the DataFrame (Normalization)\n",
        "\n",
        "def clean_text(text, mode=\"input\" ,lowercase=True):\n",
        "                                 # input (features [ product_name , brand , category , subcatogry] or the target [description])\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"<.*?>\", \" \", text) # Remove HTML tags\n",
        "    text = re.sub(r\"(https?://\\S+|www\\.\\S+|ftp://\\S+)\", \" \", text) # Remove URLs\n",
        "\n",
        "    if mode == \"description\":\n",
        "        text = re.sub(r\"[^a-z0-9\\s&-]\", \" \", text) # For descriptions: keep only letters, numbers, spaces, &, -\n",
        "\n",
        "    else:  # mode == \"input features\"\n",
        "\n",
        "        text = re.sub(r\"&\", \" & \", text) # Add spaces around &\n",
        "        text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text)  # Split camelCase or PascalCase\n",
        "        text = re.sub(r\"(\\||/|>)\", \" > \", text) # Normalize separators (|, /, >) to \" > \" as some product names contains | symbole\n",
        "\n",
        "        if lowercase: #\n",
        "           text = re.sub(r\"[^a-z0-9\\s&'\\->]\", \" \", text)  # Keep allowed chars: a-z, 0-9, spaces, &, -, >,'(lowercase for category & sub_catogry)\n",
        "        else:\n",
        "           text = re.sub(r\"[^a-zA-Z0-9\\s&'\\->]\", \" \", text) # (uppercase for product_name & brand)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize multiple spaces to single space\n",
        "    text = re.sub(r\"( > )+\", \" > \", text) # Normalize multiple > in a row\n",
        "    text = text.strip(\" >\") # Remove leading/trailing >\n",
        "\n",
        "    return text\n",
        "### apply cleaning functions on columns\n",
        "\n",
        "\"\"\"\n",
        "    Clean product, brand, category, sub_category, and description.\n",
        "    Restore original casing of product/brand inside description.\n",
        "    Remove duplicates and empty descriptions.\n",
        "\"\"\"\n",
        "def preprocess_dataset_clean_only(df):\n",
        "    clean_df = pd.DataFrame()\n",
        "\n",
        "    # Clean product_name and brand (keep original casing)\n",
        "    clean_df[\"clean_product_name\"] = df[\"product_name\"].apply(\n",
        "        lambda x: clean_text(x, mode=\"input\", lowercase=False)\n",
        "    )\n",
        "    clean_df[\"clean_brand\"] = df[\"brand\"].apply(\n",
        "        lambda x: clean_text(x, mode=\"input\", lowercase=False)\n",
        "    )\n",
        "\n",
        "    # Clean category and sub_category (convert to lowercase)\n",
        "    clean_df[\"clean_category\"] = df[\"category\"].apply(\n",
        "        lambda x: clean_text(x, mode=\"input\", lowercase=True)\n",
        "    )\n",
        "    clean_df[\"clean_sub_category\"] = df[\"sub_category\"].apply(\n",
        "        lambda x: clean_text(x, mode=\"input\", lowercase=True)\n",
        "    )\n",
        "\n",
        "    # Clean description (convert to lowercase)\n",
        "    clean_df[\"clean_description\"] = df[\"description\"].apply(\n",
        "        lambda x: clean_text(x, mode=\"description\", lowercase=True)\n",
        "    )\n",
        "\n",
        "    # Restore original casing of product name and brand inside the description\n",
        "    def restore_casing(row):\n",
        "        desc = row[\"clean_description\"]\n",
        "        for token in row[\"clean_product_name\"].split():\n",
        "            pattern = r\"\\b\" + re.escape(token.lower()) + r\"\\b\"\n",
        "            desc = re.sub(pattern, token, desc)\n",
        "        for token in row[\"clean_brand\"].split():\n",
        "            pattern = r\"\\b\" + re.escape(token.lower()) + r\"\\b\"\n",
        "            desc = re.sub(pattern, token, desc)\n",
        "        return desc\n",
        "\n",
        "    clean_df[\"clean_description\"] = clean_df.apply(restore_casing, axis=1)\n",
        "\n",
        "    # Drop duplicates and empty rows\n",
        "    clean_df = clean_df.drop_duplicates(subset=[\"clean_description\"])\n",
        "    clean_df = clean_df[clean_df[\"clean_description\"] != \"\"].reset_index(drop=True)\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "#3- function for Tokenization / Lemmatization\n",
        "\n",
        "def tokenize_lemmatize(texts, product_names, brands, nlp):\n",
        "    \"\"\"\n",
        "    Tokenize and lemmatize a list of texts using spaCy,\n",
        "    while preserving original casing for product/brand tokens.\n",
        "    Returns: list of token lists.\n",
        "    \"\"\"\n",
        "    preserve_sets = [\n",
        "        set(prod.split()) | set(br.split())\n",
        "        for prod, br in zip(product_names, brands)\n",
        "    ]\n",
        "\n",
        "    result_tokens = []\n",
        "    for doc, preserve in zip(nlp.pipe(texts, batch_size=64), preserve_sets):\n",
        "        tokens = []\n",
        "        for token in doc:\n",
        "            if token.text in preserve:\n",
        "                tokens.append(token.text)              # keep product/brand tokens\n",
        "            elif not token.is_stop and token.is_alpha:\n",
        "                tokens.append(token.lemma_)            # lemmatize normal words\n",
        "        result_tokens.append(tokens)\n",
        "\n",
        "    return result_tokens\n",
        "\n",
        "# 4) Main call to run both cleaning + tokenization   (ready for model from scratch)\n",
        "\n",
        "def process_for_model(df, nlp):\n",
        "    \"\"\"\n",
        "    Clean the dataset and add a tokenized/lemmatized version of the description.\n",
        "    \"\"\"\n",
        "    clean_df = preprocess_dataset_clean_only(df)\n",
        "    clean_df[\"description_tokens\"] = tokenize_lemmatize(\n",
        "        clean_df[\"clean_description\"].tolist(),\n",
        "        clean_df[\"clean_product_name\"].tolist(),\n",
        "        clean_df[\"clean_brand\"].tolist(),\n",
        "        nlp\n",
        "    )\n",
        "    # Drop clean_description if you only need tokens\n",
        "    # clean_df = clean_df.drop(columns=[\"clean_description\"])\n",
        "    return clean_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d19ab116"
      },
      "outputs": [],
      "source": [
        "### Download Data From Github\n",
        "\n",
        "zip_url = 'https://raw.githubusercontent.com/Eng-Shady-Hub/Generative_AI_Project_Round3/refs/heads/main/All_Datasets2.zip'\n",
        "response = requests.get(zip_url)\n",
        "response.raise_for_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVc6aWfF1Uk5"
      },
      "outputs": [],
      "source": [
        "## Reading the CSV files from the Zip and save them individually\n",
        "\n",
        "dataframes = {}\n",
        "\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "    csv_files = [f for f in z.namelist() if f.endswith(\".csv\")]\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"No CSV_Files\")\n",
        "    else:\n",
        "        for i, file_name in enumerate(csv_files, start=1):\n",
        "            with z.open(file_name) as f:\n",
        "                key = f\"df{i}\"\n",
        "                dataframes[key] = pd.read_csv(f, encoding='latin-1')\n",
        "                print(f'DataFrame \"{key}\" created from file {file_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxiJQjHC1Uk6"
      },
      "outputs": [],
      "source": [
        "# DataFrame 1\n",
        "\n",
        "basket_data = dataframes[\"df1\"]\n",
        "# data_details(basket_data)\n",
        "\n",
        "# target = [description]\n",
        "# features = [product_name ,brand ,category, sub_category]\n",
        "basket_data = basket_data[[\"product\",\"brand\",\"category\" , \"sub_category\",\"description\"]]\n",
        "basket_data= basket_data.rename(columns={\"product\": \"product_name\"})\n",
        "print(basket_data.columns)\n",
        "\n",
        "# there are null values in product_name , brand & description columns\n",
        "basket_data=basket_data.dropna(subset=[\"description\",\"product_name\",\"brand\"])\n",
        "data_details(basket_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7eTGlx61Uk6"
      },
      "outputs": [],
      "source": [
        "# DataFrame 2\n",
        "\n",
        "adidas_data = dataframes[\"df2\"]\n",
        "# data_details(adidas_data)\n",
        "\n",
        "# Unify columns names\n",
        "adidas_data = adidas_data[[\"Product Name\", \"Brand\", \"Description\"]].rename(columns={\"Product Name\": \"product_name\", \"Brand\": \"brand\" , \"Description\":\"description\"})\n",
        "adidas_data.info()\n",
        "\n",
        "# there are null values(only 3) in description column\n",
        "adidas_data=adidas_data.dropna(subset=[\"description\"])\n",
        "adidas_data =adidas_data[adidas_data['description'] != 'No description']\n",
        "adidas_data.isnull().sum()\n",
        "\n",
        "# Dataset has agood description to our model but not have category & sub_category columns\n",
        "# So we map category & sub_category columns with respect to the product_name column\n",
        "category_map = {\n",
        "    # cayrgory = footwear\n",
        "  \"shoe\": (\"footwear\", \"shoes\"),\"sneaker\": (\"footwear\", \"shoes\"),\"running\": (\"footwear\", \"running shoes\"),\"trainer\": (\"footwear\", \"trainers\"),\"cleat\": (\"footwear\", \"cleats\"),\n",
        "    \"slipper\": (\"footwear\", \"slippers\"),\"flip flop\": (\"footwear\", \"flip flops\"),\"jordan\": (\"footwear\", \"basketball shoes\"),\"retro\": (\"footwear\", \"shoes\"),\n",
        "    \"phantom\": (\"footwear\", \"cleats\"),\"venom\": (\"footwear\", \"cleats\"),\"mercurial\": (\"footwear\", \"soccer shoes\"),\"superfly\": (\"footwear\", \"soccer shoes\"),\n",
        "    \"tf\": (\"footwear\", \"turf soccer shoes\"),\"air max\": (\"footwear\", \"sneakers\"),\"p-6000\": (\"footwear\", \"running shoes\"),\"sandal\": (\"footwear\", \"sandals\"),\n",
        "    \"slide\": (\"footwear\", \"slides\"),\"adilette\": (\"footwear\", \"slides\"),\"flipflop\": (\"footwear\", \"flip flops\"),\"sb\": (\"footwear\", \"skate shoes\"),\"skate\": (\"footwear\", \"skate shoes\"),\n",
        "    \"chron\": (\"footwear\", \"skate shoes\"),\"kd\": (\"footwear\", \"basketball shoes\"),\"kyrie\": (\"footwear\", \"basketball shoes\"),\"iconclash\": (\"footwear\", \"running shoes\"),\n",
        "    \"daybreak\": (\"footwear\", \"sneakers\"),\"blazer\": (\"footwear\", \"sneakers\"),\"prelove\": (\"footwear\", \"sneakers\"),\"pegasus\": (\"footwear\", \"running shoes\"),\n",
        "    \"vaporfly\": (\"footwear\", \"running shoes\"),\"zoomx\": (\"footwear\", \"running shoes\"),\"slipon\": (\"footwear\", \"slip-ons\"),\"airforce\": (\"footwear\", \"sneakers\"),\n",
        "    \"airmax\": (\"footwear\", \"sneakers\"),\"metcon\": (\"footwear\", \"training shoes\"),\"court\": (\"footwear\", \"tennis shoes\"),\"pg\": (\"footwear\", \"basketball shoes\"),\n",
        "    \"m2k\": (\"footwear\", \"sneakers\"),\"winflo\": (\"footwear\", \"running shoes\"),\"vomero\": (\"footwear\", \"running shoes\"),\"vapormax\": (\"footwear\", \"lifestyle sneakers\"),\n",
        "    \"flip-flop\": (\"footwear\", \"flip flops\"),\"flip-flops\": (\"footwear\", \"flip flops\"),\"slip-on\": (\"footwear\", \"slip-ons\"), \"slip-ons\": (\"footwear\", \"slip-ons\"),\n",
        "    \"odyssey react\": (\"footwear\", \"running shoes\"),\"legend react\": (\"footwear\", \"running shoes\"),\"pre-love\": (\"footwear\", \"sneakers\"),\"air force\": (\"footwear\", \"sneakers\"),\n",
        "    \"drop-type\": (\"footwear\", \"running shoes\"),\"zoom rival fly\": (\"footwear\", \"running shoes\"),\"mx-720-818\": (\"footwear\", \"running shoes\"),\"tanjun\": (\"footwear\", \"running shoes\"),\n",
        "    \"superstar\": (\"footwear\", \"sneakers\"),\"slip on\": (\"footwear\", \"slip-ons\"),\"lebron soldier\": (\"footwear\", \"basketball shoes\"),\"react element\": (\"footwear\", \"running shoes\"),\n",
        "    \"free rn\": (\"footwear\", \"running shoes\"),\"zoom fly\": (\"footwear\", \"running shoes\"),\"zoom rise\": (\"footwear\", \"running shoes\"),\"tiempo legend\": (\"footwear\", \"soccer shoes\"),\n",
        "    \"flex rn\": (\"footwear\", \"running shoes\"),\"air zoom structure\": (\"footwear\", \"running shoes\"),\"sfb gen 2\": (\"footwear\", \"boots\"),\"air huarache\": (\"footwear\", \"sneakers\"),\n",
        "    \"wildhorse\": (\"footwear\", \"running shoes\"),\"benassi\": (\"footwear\", \"slides\"),\"terra kiger\": (\"footwear\", \"running shoes\"),\"classic cortez\": (\"footwear\", \"sneakers\"),\n",
        "    \"renew run\": (\"footwear\", \"running shoes\"),\"free tr\": (\"footwear\", \"training shoes\"),\"lebron\": (\"footwear\", \"basketball shoes\"),\"mowabb\": (\"footwear\", \"sneakers\"),\n",
        "    \"revolution\": (\"footwear\", \"running shoes\"),\"precision\": (\"footwear\", \"basketball shoes\"),\"shox\": (\"footwear\", \"running shoes\"),\"potential\": (\"footwear\", \"basketball shoes\"),\n",
        "    \"epic react\": (\"footwear\", \"running shoes\"), \"react city\": (\"footwear\", \"running shoes\"),\"kawa\": (\"footwear\", \"slides\"),\"joyride run\": (\"footwear\", \"running shoes\"),\n",
        "    \"joyride optik\": (\"footwear\", \"running shoes\"),\"flex contact\": (\"footwear\", \"running shoes\"),\"football\": (\"footwear\", \"Football Shoes\"),\"predator\": (\"footwear\", \"Football Shoes\"),\n",
        "    \"vandalised\": (\"footwear\", \"Casual Shoes\"),\"canyon\": (\"footwear\", \"Casual Shoes\"),\"react\": (\"footwear\", \"Running Shoes\"),\"acg\": (\"footwear\", \"Outdoor Shoes\"),\n",
        "    \"flex\": (\"footwear\", \"Training Shoes\"),\"signal\": (\"footwear\", \"Running Shoes\"),\"joyride\": (\"footwear\", \"Running Shoes\"),\"cortez\": (\"footwear\", \"Casual Shoes\"),\n",
        "    \"hawkins\": (\"footwear\", \"Casual Shoes\"),\"nemeziz\": (\"footwear\", \"Football Shoes\"),\"indoor\": (\"footwear\", \"Indoor Shoes\"),\"outdoor\": (\"footwear\", \"Outdoor Shoes\"),\n",
        "    \"trail\": (\"footwear\", \"Outdoor Shoes\"),\"superrep\": (\"footwear\", \"Training Shoes\"),\"zoom\": (\"footwear\", \"Running Shoes\"),\"tr\": (\"footwear\", \"Training Shoes\"),\n",
        "    \"renew\": (\"footwear\", \"Running Shoes\"),\"ghost\": (\"footwear\", \"Running Shoes\"),\"racer\": (\"footwear\", \"Running Shoes\"),\"alphadunk\": (\"footwear\", \"Basketball Shoes\"),\n",
        "    \"monarch\": (\"footwear\", \"Walking Shoes\"),\"af-1\": (\"footwear\", \"Casual Shoes\"),\"bella\": (\"footwear\", \"Casual Shoes\"), \"huarache\": (\"footwear\", \"Lifestyle Shoes\"),\n",
        "    \"solarsoft\": (\"footwear\", \"Training Shoes\"),\"exp-x14\": (\"footwear\", \"Running Shoes\"),\"fly.by\": (\"footwear\", \"Basketball Shoes\"),\"xarr\": (\"footwear\", \"Training Shoes\"),\n",
        "    \"skarn\": (\"footwear\", \"Casual Shoes\"),\"tailwind\": (\"footwear\", \"Running Shoes\"), \"air dsvm\": (\"footwear\", \"Running Shoes\"),\n",
        "    # category = accessories\n",
        "    \"sock\": (\"accessories\", \"socks\"), \"cap\": (\"accessories\", \"cap\"),\"hat\": (\"accessories\", \"cap\"),\"bag\": (\"accessories\", \"bag\"),\"backpack\": (\"accessories\", \"bag\"),\n",
        "    \"watch\": (\"accessories\", \"watch\")\n",
        "    }\n",
        "\n",
        "def categorize_product(name):\n",
        "    name = str(name).lower()\n",
        "    for keyword, (cat, subcat) in category_map.items():\n",
        "        if keyword in name:\n",
        "            return cat, subcat\n",
        "    return \"Other\", \"Other\"  # fallback if no keyword found\n",
        "\n",
        "adidas_data[[\"category\", \"sub_category\"]] = adidas_data[\"product_name\"].apply(lambda x: pd.Series(categorize_product(x)))\n",
        "\n",
        "# arranging the columns to be the same in all datasets\n",
        "adidas_data = adidas_data[[\"product_name\", \"brand\",\"category\", \"sub_category\", \"description\"]]\n",
        "print(adidas_data.columns)\n",
        "data_details(adidas_data , n=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKrvugcL1Uk6"
      },
      "outputs": [],
      "source": [
        "# DataFrame 3\n",
        "\n",
        "amazon_data =dataframes[\"df3\"]\n",
        "# data_details(amazon_data)\n",
        "\n",
        "# there is no null values in the prefered dataset features\n",
        "#  Amazon dataset don't contain brand , we note the first name in the product_name is the brand\n",
        "# So creating a function to map the brand column with respect to product_name column\n",
        "def map_brand(name):\n",
        "    return name.split()[0]\n",
        "\n",
        "# Apply function\n",
        "amazon_data['brand'] =amazon_data['product_name'].apply(map_brand)\n",
        "\n",
        "#  Amazon dataset don't contain sub_category , we note the values in category colums are diveded by |\n",
        "# So creating it by map sub_category column with respect to category column by extracting the most specific level(last part)\n",
        "amazon_data['sub_category'] = amazon_data['category'].apply(lambda x: x.split('|')[-1])\n",
        "# amazon_data.head()\n",
        "\n",
        "# arranging the columns to be the same in all datasets\n",
        "amazon_data = amazon_data[[\"product_name\", \"brand\",\"category\", \"sub_category\", \"about_product\"]]\n",
        "amazon_data= amazon_data.rename(columns={\"about_product\": \"description\"})\n",
        "print(amazon_data.columns)\n",
        "data_details(amazon_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Ilc8TP1Uk7"
      },
      "outputs": [],
      "source": [
        "# DataFrame 4\n",
        "\n",
        "flipkart_data = dataframes[\"df4\"]\n",
        "# data_details(flipkart_data)\n",
        "\n",
        "# there are nulls in description an brand columns\n",
        "# clearing \"discription\" rows with missed values\n",
        "flipkart_data=flipkart_data.dropna(subset=[\"description\"])\n",
        "flipkart_data =flipkart_data[flipkart_data['description'] != 'No description']\n",
        "flipkart_data.isnull().sum()\n",
        "\n",
        "# display the most common brand to fill the missing value\n",
        "most_common = flipkart_data['brand'].mode()[0]\n",
        "# print(most_common)\n",
        "\n",
        "# filling the missed value of brand By common brand in our dataset \"REEB\"\n",
        "flipkart_data['brand'] = flipkart_data['brand'].fillna('REEB')\n",
        "\n",
        "# arranging the columns to be the same in all datasets\n",
        "flipkart_data = flipkart_data[[\"title\", \"brand\",\"category\", \"sub_category\", \"description\"]]\n",
        "flipkart_data= flipkart_data.rename(columns={\"title\": \"product_name\"})\n",
        "print(flipkart_data.columns)\n",
        "flipkart_data.isnull().sum()\n",
        "data_details(flipkart_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSEgzsMK1Uk7"
      },
      "outputs": [],
      "source": [
        "# DataFrame 5\n",
        "\n",
        "adidas2_data =dataframes[\"df5\"]\n",
        "# data_details(adidas2_data)\n",
        "\n",
        "# adidas2 dataset not have null values\n",
        "#  Noting the breadcrumbs colums contains sub_category\n",
        "adidas2_data = adidas2_data[[\"name\", \"brand\",\"category\", \"breadcrumbs\", \"description\"]]\n",
        "adidas2_data= adidas2_data.rename(columns={\"name\": \"product_name\" , \"breadcrumbs\":\"sub_category\"})\n",
        "print(adidas2_data.columns)\n",
        "data_details(adidas2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RDFHguE1Uk7"
      },
      "outputs": [],
      "source": [
        "# DataFrame 6\n",
        "\n",
        "elec_data = dataframes[\"df6\"]\n",
        "# data_details(elec_data)\n",
        "\n",
        "# The elec_data dataSet is clear\n",
        "elec_data= elec_data.rename(columns={\"Product_name\": \"product_name\"})\n",
        "print(elec_data.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K67Skzu1Uk8"
      },
      "outputs": [],
      "source": [
        "# DataFrame 7\n",
        "\n",
        "Bigbasket2 = dataframes[\"df7\"]\n",
        "data_details(Bigbasket2)\n",
        "Bigbasket2=Bigbasket2[[\"SKU Name\",\"Brand\",\"Category\",\"Sub-Category\",\"About the Product\"]]\n",
        "data_details(Bigbasket2)\n",
        "# rename the column and clean the row with null or no description\n",
        "\n",
        "Bigbasket2=Bigbasket2.rename(columns={\"SKU Name\": \"product_name\" , \"Brand\":\"brand\",\"Category\":\"category\", \"Sub-Category\":\"sub_category\" ,\"About the Product\":\"description\"})\n",
        "Bigbasket2=Bigbasket2.dropna(subset=[\"description\"])\n",
        "Bigbasket2 =Bigbasket2[Bigbasket2['description'] != 'No description']\n",
        "Bigbasket2.isnull().sum()\n",
        "\n",
        "# the brand column has only 3 null values\n",
        "\n",
        "Bigbasket2=Bigbasket2.dropna(subset=[\"brand\"])\n",
        "\n",
        "data_details(Bigbasket2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKFse8VN1Uk8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Combining all datasets\n",
        "\n",
        "Final_data = pd.concat([basket_data, adidas_data, amazon_data ,adidas2_data , flipkart_data , elec_data  ,Bigbasket2], ignore_index=True)\n",
        "data_details(Final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjJdE0B91Uk8"
      },
      "outputs": [],
      "source": [
        "## Saving Final DataSet\n",
        "save_path = os.path.expanduser(\"~/Documents/Final_data.csv\")\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True) # Create directory if it doesn't exist\n",
        "Final_data.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset Saved to : \", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be6Ss3wH1Uk8"
      },
      "outputs": [],
      "source": [
        "# Applying on Data, Just clean (ready for Fine-Tune Pre-trained Model)\n",
        "\n",
        "clean_Final_data = preprocess_dataset_clean_only(Final_data)\n",
        "data_details(clean_Final_data)\n",
        "#Saving to CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hapnq1oX1Uk8"
      },
      "outputs": [],
      "source": [
        "#Saving to CSV file\n",
        "save_path = os.path.expanduser(\"~/Documents/clean_Final_data.csv\")\n",
        "clean_Final_data.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset Saved to : \", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJGlU9wQ1Uk8"
      },
      "outputs": [],
      "source": [
        "#  Applying on Data, Clean + tokens (ready for model from scratch)\n",
        "\n",
        "clean_Final_data_model = process_for_model(Final_data, nlp)\n",
        "clean_Final_data_model.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS9aowrd1Uk9"
      },
      "outputs": [],
      "source": [
        "#Saving to CSV file\n",
        "save_path = os.path.expanduser(\"~/Documents/clean_Final_data_model.csv\")\n",
        "clean_Final_data_model.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset Saved to : \", save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbOZl5Q51Uk9"
      },
      "outputs": [],
      "source": [
        "### Visualization ###\n",
        "\n",
        "\n",
        "all_words = \" \".join(clean_Final_data[\"clean_description\"]).split()\n",
        "word_counts = Counter(all_words)\n",
        "common_words = word_counts.most_common(20)\n",
        "\n",
        "# Bar chart most frequent 20 words\n",
        "words, counts = zip(*common_words)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(words[::-1], counts[::-1])\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.title(\"Top 20 Words\")\n",
        "plt.show()\n",
        "\n",
        "# WordCloud\n",
        "wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_counts)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fBrTUpg1Uk9"
      },
      "outputs": [],
      "source": [
        "clean_Final_data_model = process_for_model(Final_data, nlp)\n",
        "save_path = os.path.expanduser(\"~/Documents/clean_Final_data_model.csv\")\n",
        "clean_Final_data_model.to_csv(save_path, index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset Saved to : \", save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyLcTJyi1Uk9"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, pipeline\n",
        "from nltk.corpus import wordnet\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD1Z663F1Uk9"
      },
      "outputs": [],
      "source": [
        "for pkg in (\"punkt\", \"wordnet\", \"omw-1.4\"):\n",
        "    try:\n",
        "        nltk.data.find(f\"corpora/{pkg}\")\n",
        "    except LookupError:\n",
        "        print(f\"Downloading NLTK package: {pkg} ...\")\n",
        "        nltk.download(pkg)\n",
        "\n",
        "\n",
        "def get_word_info(word: str):\n",
        "    \"\"\"Return a dict with definition, examples (list), and synonyms (list). Always returns 'synonyms' key.\"\"\"\n",
        "    try:\n",
        "        synsets = wordnet.synsets(word)\n",
        "    except Exception as e:\n",
        "        print(\"Warning: error accessing WordNet:\", str(e))\n",
        "        try:\n",
        "            nltk.download(\"wordnet\")\n",
        "            synsets = wordnet.synsets(word)\n",
        "        except Exception as e2:\n",
        "            print(\"Failed to access WordNet after download:\", e2)\n",
        "            return {\"definition\": f\"No definition found for '{word}'.\", \"examples\": [], \"synonyms\": []}\n",
        "\n",
        "    if not synsets:\n",
        "        return {\"definition\": f\"No definition found for '{word}'.\", \"examples\": [], \"synonyms\": []}\n",
        "\n",
        "    s = synsets[0]\n",
        "    meaning = s.definition() or f\"No definition found for '{word}'.\"\n",
        "    examples = s.examples() or []\n",
        "    synonyms = [lemma.name() for lemma in s.lemmas()] or []\n",
        "    return {\"definition\": meaning, \"examples\": examples, \"synonyms\": synonyms}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JR4Q02Yk1Uk9"
      },
      "outputs": [],
      "source": [
        "def get_word_info(word):\n",
        "    \"\"\"\n",
        "    Get the definition, examples, and synonyms of a word using WordNet.\n",
        "    \"\"\"\n",
        "    synsets = wordnet.synsets(word)\n",
        "    if not synsets:\n",
        "        return {\"definition\": f\"No definition found for {word}.\", \"examples\": []}\n",
        "\n",
        "    meaning = synsets[0].definition()\n",
        "    examples = synsets[0].examples()\n",
        "    synonyms = [lemma.name() for lemma in synsets[0].lemmas()]\n",
        "    return {\"definition\": meaning, \"examples\": examples, \"synonyms\": synonyms}\n",
        "    if not synsets:\n",
        "        return {\"definition\": f\"No definition found for '{word}'.\", \"examples\": [], \"synonyms\": []}\n",
        "\n",
        "    s = synsets[0]\n",
        "    meaning = s.definition() or f\"No definition found for '{word}'.\"\n",
        "    examples = s.examples() or []\n",
        "    synonyms = [lemma.name() for lemma in s.lemmas()] or []\n",
        "    return {\"definition\": meaning, \"examples\": examples, \"synonyms\": synonyms}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QZqDV3Zk1Uk-"
      },
      "outputs": [],
      "source": [
        "def clean_repetitions(text):\n",
        "    text = re.sub(r'(\\b\\w+\\b(?:\\s+\\b\\w+\\b){1,3})\\s+\\1+', r'\\1', text)\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    seen = set()\n",
        "    result = []\n",
        "    for s in sentences:\n",
        "        if s.lower().strip() not in seen:\n",
        "            result.append(s.strip())\n",
        "            seen.add(s.lower().strip())\n",
        "    return ' '.join(result).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4v2iP7T-1Uk-"
      },
      "outputs": [],
      "source": [
        "def generate_description(word: str, definition: str):\n",
        "    \"\"\"Use the pipeline; return a safe string on any error or if model not loaded.\"\"\"\n",
        "    if pipe is None:\n",
        "        return \"(generation model not loaded)\"\n",
        "    prompt = f\"The word '{word}' refers to {definition}. It is commonly known as\"\n",
        "    try:\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            max_length=80,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        if isinstance(out, list) and len(out) > 0:\n",
        "            return out[0].get(\"generated_text\", \"(no generated_text key)\")\n",
        "        return str(out)\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "        return \"(generation failed)\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji2AB2b01Uk-"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Interactive GPT-2 + WordNet (type 'exit' to quit)\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter a word or short phrase: \").strip()\n",
        "        if user_input.lower() in (\"exit\", \"quit\"):\n",
        "            print(\"Goodbye ðŸ‘‹\")\n",
        "            break\n",
        "        if not user_input:\n",
        "            print(\"Please type something (or 'exit').\")\n",
        "            continue\n",
        "\n",
        "        info = get_word_info(user_input)\n",
        "        print(\"\\nDefinition:\")\n",
        "        print(\" \", info.get(\"definition\", \"(none)\"))\n",
        "\n",
        "        syns = info.get(\"synonyms\") or []\n",
        "        if syns:\n",
        "            # make sure all syns are strings\n",
        "            syns = [str(s) for s in syns]\n",
        "            print(\"Synonyms:\", \", \".join(syns))\n",
        "        else:\n",
        "            print(\"Synonyms: None found\")\n",
        "\n",
        "        examples = info.get(\"examples\") or []\n",
        "        if examples:\n",
        "            print(\"Example:\", examples[0])\n",
        "        else:\n",
        "            print(\"Example: None found\")\n",
        "\n",
        "        print(\"\\nGenerating AI description...\")\n",
        "        result = generate_description(user_input, info.get(\"definition\", \"\"))\n",
        "        print(\"\\nGPT-2 result:\\n\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-yAiGYH1Uk-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "depi-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}