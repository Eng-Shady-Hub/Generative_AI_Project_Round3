{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc5eab9f",
        "outputId": "ff522260-a877-4663-d4b4-2e87424c12c8"
      },
      "outputs": [],
      "source": [
        "#!pip install openpyxl\n",
        "#!pip install spacy\n",
        "#!pip install requests\n",
        "#!pip install pandas\n",
        "#!pip install torch torchvision torchaudio\n",
        "#!pip install scikit-learn\n",
        "#!pip install openpyxl\n",
        "#!pip install gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06d3d702"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAVlbMY9FIdH"
      },
      "outputs": [],
      "source": [
        "# Reproducibility & device\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5vU1fzfA2cf"
      },
      "outputs": [],
      "source": [
        "####    Genaral Functions   ####\n",
        "\n",
        "# 1- Function for showing the dataframe characteristics\n",
        "\n",
        "def data_details(df, n=5):\n",
        "\n",
        "     print(\"\\n\")\n",
        "     print(\"Shape:\")\n",
        "     print(df.shape)\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n The Head\")\n",
        "     display(df.head(n))\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n Info:\")\n",
        "     print(df.info(memory_usage=\"deep\"))\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\n The Null Values:\")\n",
        "     print(df.isnull().sum())\n",
        "     print(\"\\n\")\n",
        "\n",
        "     print(\"\\nSummary statistics (categorical):\")  # As our needed features are categorical\n",
        "     categorical_cols = df.select_dtypes(include=[object]).columns\n",
        "     if len(categorical_cols) > 0:\n",
        "         display(df.describe(include=[object]))\n",
        "     else:\n",
        "         print(\"No categorical columns found.\")\n",
        "         print(\"\\n\")\n",
        "\n",
        "# 2- Function for cleaning the DataFrame (Normalization)\n",
        "\n",
        "def clean_text(text, mode=\"input\" ,lowercase=True):\n",
        "                                 # input (features [ product_name , brand , category , subcatogry] or the target [description])\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"<.*?>\", \" \", text) # Remove HTML tags\n",
        "    text = re.sub(r\"(https?://\\S+|www\\.\\S+|ftp://\\S+)\", \" \", text) # Remove URLs\n",
        "\n",
        "    if mode == \"description\":\n",
        "        text = re.sub(r\"[^a-z0-9\\s&-]\", \" \", text) # For descriptions: keep only letters, numbers, spaces, &, -\n",
        "\n",
        "    else:  # mode == \"input features\"\n",
        "\n",
        "        text = re.sub(r\"&\", \" & \", text) # Add spaces around &\n",
        "        text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text)  # Split camelCase or PascalCase\n",
        "        text = re.sub(r\"(\\||/|>)\", \" > \", text) # Normalize separators (|, /, >) to \" > \" as some product names contains | symbole\n",
        "\n",
        "        if lowercase: #\n",
        "           text = re.sub(r\"[^a-z0-9\\s&'\\->]\", \" \", text)  # Keep allowed chars: a-z, 0-9, spaces, &, -, >,'(lowercase for category & sub_catogry)\n",
        "        else:\n",
        "           text = re.sub(r\"[^a-zA-Z0-9\\s&'\\->]\", \" \", text) # (uppercase for product_name & brand)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize multiple spaces to single space\n",
        "    text = re.sub(r\"( > )+\", \" > \", text) # Normalize multiple > in a row\n",
        "    text = text.strip(\" >\") # Remove leading/trailing >\n",
        "\n",
        "    return text\n",
        "\n",
        "# 3- function to apply tokenization - lemmatization - stopword/punctuation removal\n",
        "# and keep the original casing for product names and brand\n",
        "\n",
        "def tokenize_lemmatize(text, product_name=None, brand=None):\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    preserve_tokens = set() # a set containing all the parts (tokens) of product name and brand.\n",
        "    if product_name:\n",
        "        preserve_tokens.update(product_name.split())\n",
        "    if brand:\n",
        "        preserve_tokens.update(brand.split())\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.text in preserve_tokens:\n",
        "            tokens.append(token.text)  # keep original casing\n",
        "        elif not token.is_stop and token.is_alpha:\n",
        "            tokens.append(token.lemma_)  # lemmatize normal words\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "# 4- function to apply the clean_text and tokenize_lemmatize on our dataframe \n",
        "def preprocess_dataset_clean_only(df, for_model=False):\n",
        "\n",
        "    clean_df = pd.DataFrame() # To return a new DataFrame with only cleaned columns\n",
        "\n",
        "    feature_cols = [\"product_name\", \"brand\"] # Clean feature columns\n",
        "    for col in feature_cols:\n",
        "        clean_df[f\"clean_{col}\"] = df[col].apply(lambda x: clean_text(x, mode=\"input\",lowercase=False )) # keep upercase\n",
        "\n",
        "    feature_cols = [ \"category\", \"sub_category\"]  # Clean feature columns\n",
        "    for col in feature_cols:\n",
        "        clean_df[f\"clean_{col}\"] = df[col].apply(lambda x: clean_text(x, mode=\"input\",lowercase=True )) # convert to lowercase\n",
        "\n",
        "\n",
        "    clean_df[\"clean_description\"] = df[\"description\"].apply(lambda x: clean_text(x, mode=\"description\", lowercase=True )) # Clean description\n",
        "\n",
        "#  Replace lowercase product/brand mentions with their original casing that is mentioned in description\n",
        "    for i, row in clean_df.iterrows():\n",
        "         desc = clean_df.at[i, \"clean_description\"]\n",
        "\n",
        "      # Handle product name parts\n",
        "         product_name = row[\"clean_product_name\"]\n",
        "         if product_name:\n",
        "             for token in product_name.split():\n",
        "                 pattern = r\"\\b\" + re.escape(token.lower()) + r\"\\b\"\n",
        "                 desc = re.sub(pattern, token, desc)\n",
        "\n",
        "     # Handle brand\n",
        "         brand = row[\"clean_brand\"]\n",
        "         if brand:\n",
        "             for token in brand.split():\n",
        "                 pattern = r\"\\b\" + re.escape(token.lower()) + r\"\\b\"\n",
        "                 desc = re.sub(pattern, token, desc)\n",
        "\n",
        "         clean_df.at[i, \"clean_description\"] = desc\n",
        "\n",
        "    clean_df = clean_df.drop_duplicates(subset=[\"clean_description\"])  # Drop duplicates and empty descriptions\n",
        "    clean_df = clean_df[clean_df[\"clean_description\"] != \"\"].reset_index(drop=True)\n",
        "\n",
        "    # Merge 4 input features into one text\n",
        "\n",
        "    clean_df[\"combined_input\"] = (\n",
        "        clean_df[\"clean_product_name\"] + \" \" +\n",
        "        clean_df[\"clean_brand\"] + \" \" +\n",
        "        clean_df[\"clean_category\"] + \" \" +\n",
        "        clean_df[\"clean_sub_category\"]\n",
        "    ).str.strip()\n",
        "\n",
        "    clean_df = clean_df[[\"combined_input\", \"clean_description\"]].copy()\n",
        "\n",
        "    # If preparing for model... add tokenization + lemmatization\n",
        "\n",
        "    if for_model:\n",
        "        clean_df[\"input_tokens\"] = clean_df.apply(\n",
        "            lambda row: tokenize_lemmatize(\n",
        "                row[\"combined_input\"], row[\"clean_product_name\"], row[\"clean_brand\"]\n",
        "            ),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        clean_df[\"description_tokens\"] = clean_df.apply(\n",
        "            lambda row: tokenize_lemmatize(\n",
        "                row[\"clean_description\"], row[\"clean_product_name\"], row[\"clean_brand\"]\n",
        "            ),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "       \n",
        "        clean_df = clean_df[[\"input_tokens\", \"description_tokens\"]].copy()\n",
        "    else:\n",
        "      clean_df = clean_df[[\"combined_input\", \"clean_description\"]].copy()\n",
        "\n",
        "    return clean_df\n",
        "    \n",
        "\n",
        "# 5- Function acts as a data preparation pipeline that transforms raw, messy text data into the structured, numerical format required for training the GAN model.  \n",
        "\n",
        "def preprocess_and_split_data(df, preprocessor, test_size=0.2, val_size=0.1, random_state=42, max_len_target=200):\n",
        "\n",
        "\n",
        "    print(\"\\n Starting preprocessing and splitting...\\n\")\n",
        "\n",
        "    # Step 1: Clean and tokenize using your existing function\n",
        "    clean_df = preprocess_dataset_clean_only(df, for_model=True)\n",
        "    print(f\"Cleaning and tokenization complete. Rows: {len(clean_df)}\")\n",
        "\n",
        "    # Step 2: Build vocabulary from both input and description tokens\n",
        "    preprocessor.build_vocab(clean_df[\"input_tokens\"].tolist() + clean_df[\"description_tokens\"].tolist())\n",
        "    print(f\"Vocabulary built. Vocab size: {len(preprocessor.word2idx)}\")\n",
        "\n",
        "    # Step 3: Convert tokens to encoded IDs\n",
        "    input_ids = [preprocessor.encode(tokens) for tokens in clean_df[\"input_tokens\"]]\n",
        "    desc_ids = [preprocessor.encode(tokens) for tokens in clean_df[\"description_tokens\"]]\n",
        "\n",
        "    # Step 4: Split data into train, validation, and test before padding\n",
        "    train_inputs, test_inputs, train_desc, test_desc = train_test_split(\n",
        "        input_ids, desc_ids, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_ratio = val_size / (1 - test_size)\n",
        "    train_inputs, val_inputs, train_desc, val_desc = train_test_split(\n",
        "        train_inputs, train_desc, test_size=val_ratio, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Step 5: Pad sequences\n",
        "    train_inputs = preprocessor.pad_sequences(train_inputs, max_len=max_len_target)\n",
        "    val_inputs = preprocessor.pad_sequences(val_inputs, max_len=max_len_target)\n",
        "    test_inputs = preprocessor.pad_sequences(test_inputs, max_len=max_len_target)\n",
        "\n",
        "\n",
        "    train_desc = preprocessor.pad_sequences(train_desc, max_len=max_len_target)\n",
        "    val_desc = preprocessor.pad_sequences(val_desc, max_len=max_len_target)\n",
        "    test_desc = preprocessor.pad_sequences(test_desc, max_len=max_len_target)\n",
        "\n",
        "    # Step 6: Create TensorDatasets (for PyTorch models)\n",
        "    train_ds = torch.utils.data.TensorDataset(train_inputs, train_desc)\n",
        "    val_ds = torch.utils.data.TensorDataset(val_inputs, val_desc)\n",
        "    test_ds = torch.utils.data.TensorDataset(test_inputs, test_desc)\n",
        "\n",
        "    print(f\"\\n Data split complete:\")\n",
        "    print(f\"\\n Train set: {len(train_ds)} samples\")\n",
        "    print(f\"\\n Validation set: {len(val_ds)} samples\")\n",
        "    print(f\"\\n Test set: {len(test_ds)} samples\")\n",
        "\n",
        "    # Step 7: Return everything needed for training\n",
        "    return {\n",
        "        \"train\": train_ds,\n",
        "        \"val\": val_ds,\n",
        "        \"test\": test_ds,\n",
        "        \"vocab_size\": len(preprocessor.word2idx),\n",
        "        \"max_len\": train_inputs.size(1),\n",
        "        \"clean_df\": clean_df\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdRDyMh-FIdM"
      },
      "outputs": [],
      "source": [
        "#  Word2Vec Training Separately\n",
        "\n",
        "def train_word2vec(clean_df, vector_size=128, window=5, min_count=1, epochs=10):\n",
        "    all_tokens = clean_df[\"input_tokens\"].tolist() + clean_df[\"description_tokens\"].tolist()\n",
        "    w2v_model = Word2Vec(\n",
        "        sentences=all_tokens,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        workers=4,\n",
        "        sg=1,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    print(\"Word2Vec training complete. Vocab size:\", len(w2v_model.wv))\n",
        "    print(w2v_model.wv)\n",
        "    return w2v_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H79ZGPzOA2cg"
      },
      "outputs": [],
      "source": [
        "## This is class to serve as the Vocabulary Manager and Sequence Formatter for GAN\n",
        "\n",
        "class TextGANPreprocessor:\n",
        "    def __init__(self, min_freq=1):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<START>\": 1, \"<END>\": 2, \"<UNK>\": 3}\n",
        "        self.idx2word = {}\n",
        "        self.min_freq = min_freq\n",
        "        self.word_freq = {}\n",
        "\n",
        "    def build_vocab(self, token_lists):\n",
        "        for tokens in token_lists:\n",
        "            for t in tokens:\n",
        "                self.word_freq[t] = self.word_freq.get(t, 0) + 1\n",
        "        for word, freq in self.word_freq.items():\n",
        "            if freq >= self.min_freq and word not in self.word2idx:\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        return [self.word2idx.get(t, self.word2idx[\"<UNK>\"]) for t in tokens]\n",
        "\n",
        "\n",
        "    def pad_sequences(self, sequences, max_len=200): # Set default to 200\n",
        "        if max_len is None: \n",
        "             max_len = max(len(s) for s in sequences)\n",
        "\n",
        "    # Ensure we don't exceed 200 if data is longer (it will be truncated)\n",
        "        padded = [s[:max_len] + [0] * max(0, max_len - len(s)) for s in sequences]\n",
        "        return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "   # helper for generation & prompts\n",
        "    def encode_text(self, text):\n",
        "        toks = tokenize_lemmatize(clean_text(text, mode=\"input\", lowercase=True), None, None)\n",
        "        return self.encode(toks)\n",
        "\n",
        "    def decode_text(self, ids):\n",
        "        words = []\n",
        "        for idx in ids:\n",
        "            w = self.idx2word.get(int(idx), \"<UNK>\")\n",
        "            if w in (\"<PAD>\", \"<START>\", \"<END>\"):\n",
        "                continue\n",
        "            words.append(w)\n",
        "        return \" \".join(words)\n",
        "\n",
        "## Generator class to generate realistic text sequences (product descriptions) conditioned on specific features  and random noise.        \n",
        "\n",
        "class Generator(nn.Module):\n",
        "     def __init__(self, vocab_size, embed_dim, hidden_dim, cond_dim, max_len, embedding_matrix=None, freeze_embeddings=False, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.cond_dim = cond_dim\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embed.weight.data.copy_(embedding_matrix)\n",
        "            self.embed.weight.requires_grad = not freeze_embeddings\n",
        "\n",
        "        # LSTM Encoder for the conditional sequence\n",
        "        self.cond_encoder = nn.LSTM(embed_dim, hidden_dim // 2,batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Linear projection to initial hidden state\n",
        "        self.cond_fc = nn.Linear(hidden_dim + cond_dim, hidden_dim)\n",
        "\n",
        "        # Decoder LSTM for text generation\n",
        "        self.num_layers = 2\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, dropout=dropout, num_layers=self.num_layers)\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "     def forward(self, noise, cond_seq, teacher_forcing_ratio=0.0, targets=None):\n",
        "        device = cond_seq.device\n",
        "        batch_size = cond_seq.size(0)\n",
        "\n",
        "        # --- Encode condition sequence ---\n",
        "        cond_emb = self.embed(cond_seq)\n",
        "        _, (h, c) = self.cond_encoder(cond_emb)\n",
        "        cond_vec = torch.cat((h[-2], h[-1]), dim=1)\n",
        "\n",
        "        combined = torch.cat((cond_vec, noise), dim=1)\n",
        "        h0_single = torch.tanh(self.cond_fc(combined))\n",
        "\n",
        "        h0 = h0_single.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n",
        "\n",
        "        # --- Start token ---\n",
        "        start_token_idx = torch.full((batch_size, 1), 1, dtype=torch.long, device=device) # <START>\n",
        "\n",
        "        # We will feed the *embedding* in the loop, not the index\n",
        "        next_input_emb = self.embed(start_token_idx)\n",
        "        h, c = h0, c0\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(self.max_len):\n",
        "            # Use the embedding from the *previous* step\n",
        "            out, (h, c) = self.lstm(next_input_emb, (h, c))\n",
        "            logits = self.fc_out(out[:, -1, :]) # Get logits [B, V]\n",
        "            outputs.append(logits)\n",
        "\n",
        "            # Decide on the *next* input\n",
        "            use_teacher_force = targets is not None and torch.rand(1).item() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_force:\n",
        "                # Teacher Forcing \n",
        "                next_token_idx = targets[:, t].unsqueeze(1)\n",
        "                next_input_emb = self.embed(next_token_idx)\n",
        "            else:\n",
        "                # Get soft probabilities\n",
        "                soft_probs = F.gumbel_softmax(logits, tau=1.0, hard=False, dim=1) \n",
        "                next_input_emb = torch.matmul(soft_probs, self.embed.weight).unsqueeze(1) \n",
        "\n",
        "        logits_stack = torch.stack(outputs, dim=1) \n",
        "        return logits_stack\n",
        "\n",
        "\n",
        "# Discriminator class acts as the adversary to the Generator, trained to distinguish between real (authentic product descriptions) and fake (Generator-produced) text sequences.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, embedding_matrix=None, freeze_embeddings=False):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        if embedding_matrix is not None:\n",
        "            self.embed.weight.data.copy_(embedding_matrix)\n",
        "            self.embed.weight.requires_grad = not freeze_embeddings\n",
        "\n",
        "        # Bidirectional LSTM encoder\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Classification layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    # *** CORRECTED FORWARD ***\n",
        "    def forward(self, seq):\n",
        "        if seq.dim() == 2:\n",
        "            emb = self.embed(seq) \n",
        "        elif seq.dim() == 3:\n",
        "             emb = torch.matmul(seq, self.embed.weight)\n",
        "        else:\n",
        "            raise ValueError(f\"Discriminator input has wrong dimension: {seq.dim()}\")\n",
        "\n",
        "        _, (h, _) = self.lstm(emb)              \n",
        "        hidden = torch.cat((h[-2], h[-1]), dim=1) \n",
        "        out = self.fc(hidden)                     \n",
        "        return out\n",
        "\n",
        "\n",
        "# Training Loop of GAN\n",
        "\n",
        "def train_gan(generator, discriminator, dataloader, num_epochs, vocab_size, cond_dim, preprocessor, lr_G=2e-4, lr_D=1e-4):\n",
        "\n",
        "    device = next(generator.parameters()).device\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Handles sigmoid internally\n",
        "\n",
        "\n",
        "    optim_G = torch.optim.Adam(generator.parameters(), lr=lr_G, betas=(0.5, 0.999)) #  Faster\n",
        "    optim_D = torch.optim.Adam(discriminator.parameters(), lr=lr_D, betas=(0.5, 0.999)) # Slower\n",
        "\n",
        "    best_g_loss = float('inf')  # Track best generator loss\n",
        "    gen_steps = 2  #  train generator more often\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        generator.train()\n",
        "        discriminator.train()\n",
        "\n",
        "        G_loss_total, D_loss_total = 0.0, 0.0  # reset every epoch\n",
        "\n",
        "        for i, (input_seq, real_desc) in enumerate(dataloader):\n",
        "            input_seq = input_seq.to(device)\n",
        "            real_desc = real_desc.to(device)\n",
        "            batch_size = input_seq.size(0)\n",
        "\n",
        "            # Label smoothing\n",
        "            real_labels = torch.full((batch_size, 1), 0.9, device=device)\n",
        "            fake_labels = torch.full((batch_size, 1), 0.0, device=device)\n",
        "\n",
        "            #  Train Discriminator \n",
        "            optim_D.zero_grad()\n",
        "\n",
        "            # Real samples\n",
        "            real_output = discriminator(real_desc)\n",
        "            loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "            # Generate fake samples\n",
        "            noise = torch.randn(batch_size, cond_dim, device=device)\n",
        "            with torch.no_grad(): # Don't need G grads here\n",
        "                fake_logits = generator(noise, input_seq)\n",
        "\n",
        "            fake_one_hot = F.gumbel_softmax(fake_logits, tau=1.0, hard=True, dim=2)\n",
        "\n",
        "            # Fake samples (detached, as we're training D)\n",
        "            fake_output = discriminator(fake_one_hot.detach())\n",
        "            loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "            # Total discriminator loss\n",
        "            loss_D = (loss_real + loss_fake) / 2\n",
        "            loss_D.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 5.0)  # gradient clipping\n",
        "            optim_D.step()\n",
        "\n",
        "            # Train Generator \n",
        "            for _ in range(gen_steps):\n",
        "                optim_G.zero_grad()\n",
        "                noise = torch.randn(batch_size, cond_dim, device=device)\n",
        "\n",
        "                # Generate fake logits\n",
        "                fake_logits = generator(noise, input_seq)\n",
        "\n",
        "                # *** CORRECTION: Use Gumbel-Softmax for differentiable output ***\n",
        "                # We need gradients to flow back, so no .detach()\n",
        "                fake_one_hot = F.gumbel_softmax(fake_logits, tau=1.0, hard=True, dim=2)\n",
        "\n",
        "                # Get discriminator's opinion\n",
        "                fake_output = discriminator(fake_one_hot)\n",
        "\n",
        "                # Calculate loss (how well G fooled D)\n",
        "                loss_G = criterion(fake_output, real_labels) # G wants D to think these are real\n",
        "                loss_G.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(generator.parameters(), 5.0)  # clip gradients\n",
        "                optim_G.step()\n",
        "\n",
        "            # Accumulate for logging\n",
        "            G_loss_total += loss_G.item()\n",
        "            D_loss_total += loss_D.item()\n",
        "\n",
        "            # Optional: print every 100 batches\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"   [Batch {i+1}/{len(dataloader)}] D Loss: {loss_D.item():.4f} | G Loss: {loss_G.item():.4f}\")\n",
        "\n",
        "        # Save best generator & discriminator model ---\n",
        "        avg_G = G_loss_total / len(dataloader)\n",
        "        avg_D = D_loss_total / len(dataloader)\n",
        "\n",
        "        if avg_G < best_g_loss:\n",
        "            best_g_loss = avg_G\n",
        "            torch.save(generator.state_dict(), \"best_generator.pth\")\n",
        "            torch.save(discriminator.state_dict(), \"best_discriminator.pth\")\n",
        "            # Print average loss\n",
        "            print(f\"Saved Best Models at Epoch {epoch+1} (G Loss: {avg_G:.4f})\")\n",
        "\n",
        "        # Epoch summary \n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {avg_D:.4f} | G Loss: {avg_G:.4f}\")\n",
        "\n",
        "        # Generate a sample after each epoch \n",
        "        generator.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                # Get a sample from the dataloader (use first batch)\n",
        "                sample_input, _ = next(iter(dataloader))\n",
        "                sample_input = sample_input.to(device)\n",
        "                noise = torch.randn(1, cond_dim, device=device) # Use 1 sample\n",
        "\n",
        "                fake_logits = generator(noise, sample_input[:1])\n",
        "                fake_ids = torch.argmax(fake_logits, dim=2)\n",
        "\n",
        "                generated_text = preprocessor.decode_text(fake_ids[0].tolist())\n",
        "                print(f\"Example generated description after Epoch {epoch+1}:\")\n",
        "                print(generated_text)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate sample: {e}\")\n",
        "        generator.train()\n",
        "\n",
        "\n",
        "# Generate Descriptions After Training\n",
        "\n",
        "def generate_description(generator, preprocessor, input_seq, cond_dim, device, max_len=50):\n",
        "    \"\"\"\n",
        "    Generate a product description from an input sequence using the trained Generator.\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        noise = torch.randn(1, cond_dim, device=device)\n",
        "\n",
        "        fake_logits = generator(noise, input_seq)\n",
        "\n",
        "        # We use argmax here because we are in inference, no gradients needed\n",
        "        fake_ids = torch.argmax(fake_logits, dim=2)[0].tolist()\n",
        "\n",
        "        # Decode and truncate\n",
        "        generated_text = preprocessor.decode_text(fake_ids)\n",
        "        return \" \".join(generated_text.split()[:max_len]) # Truncate to max_len words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d19ab116"
      },
      "outputs": [],
      "source": [
        "### Download Data From Github\n",
        "\n",
        "zip_url = 'https://raw.githubusercontent.com/Eng-Shady-Hub/Generative_AI_Project_Round3/refs/heads/main/All_Datasets2.zip'\n",
        "response = requests.get(zip_url)\n",
        "response.raise_for_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnF5p2r7cNdi",
        "outputId": "dda00a6a-0a71-4bc4-845d-ae6e72c38530"
      },
      "outputs": [],
      "source": [
        "dataframes = {}\n",
        "\n",
        "# Open the ZIP file from memory\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "    # Collect only CSV files\n",
        "    csv_files = [f for f in z.namelist() if f.lower().endswith(\".csv\")]\n",
        "\n",
        "    if not csv_files:\n",
        "        print(\"No CSV files found in the ZIP.\")\n",
        "    else:\n",
        "        for i, file_name in enumerate(csv_files, start=1):\n",
        "            key = f\"df{i}\"\n",
        "            try:\n",
        "                with z.open(file_name) as f:\n",
        "                    # Try UTF-8 first; fallback to latin1 if decoding fails\n",
        "                    try:\n",
        "                        dataframes[key] = pd.read_csv(f, encoding='utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        f.seek(0)\n",
        "                        dataframes[key] = pd.read_csv(f, encoding='latin1')\n",
        "\n",
        "                    print(f'DataFrame \"{key}\" created from file: {file_name} (shape: {dataframes[key].shape})')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1VIBTNuA2ci"
      },
      "outputs": [],
      "source": [
        "# Dataframe 1\n",
        "basket_data = dataframes[\"df1\"]\n",
        "data_details(basket_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vllI455HA2ci",
        "outputId": "95307cd6-0702-4481-dd64-8fabecc0ced6"
      },
      "outputs": [],
      "source": [
        "# target = [description]\n",
        "# features = [product_name ,brand ,category, sub_category]\n",
        "\n",
        "basket_data = basket_data[[\"product\",\"brand\",\"category\" , \"sub_category\",\"description\"]]\n",
        "basket_data= basket_data.rename(columns={\"product\": \"product_name\"})\n",
        "\n",
        "print(basket_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BpnKZ-WbA2ci",
        "outputId": "1302adc6-be5e-44c1-b384-21d60e11c081"
      },
      "outputs": [],
      "source": [
        "# there are null values in product_name , brand & description columns\n",
        "\n",
        "basket_data=basket_data.dropna(subset=[\"description\",\"product_name\",\"brand\"])\n",
        "data_details(basket_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p5-NJYcfA2cj",
        "outputId": "ac0729fc-1299-42ce-fb8b-332e2910b543"
      },
      "outputs": [],
      "source": [
        "# DataFrame 2\n",
        "\n",
        "adidas_data = dataframes[\"df2\"]\n",
        "data_details(adidas_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "eRTuVwUlA2cj",
        "outputId": "bda954a3-35e1-4c5d-a9e0-fdd98b289a53"
      },
      "outputs": [],
      "source": [
        "# Unify columns names\n",
        "adidas_data = adidas_data[[\"Product Name\", \"Brand\", \"Description\"]].rename(columns={\"Product Name\": \"product_name\", \"Brand\": \"brand\" , \"Description\":\"description\"})\n",
        "adidas_data.info()\n",
        "\n",
        "# there are null values(only 3) in description column\n",
        "\n",
        "adidas_data=adidas_data.dropna(subset=[\"description\"])\n",
        "adidas_data =adidas_data[adidas_data['description'] != 'No description']\n",
        "adidas_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQhK_whNA2ck"
      },
      "outputs": [],
      "source": [
        "# Dataset has agood description to our model but not have category & sub_category columns\n",
        "# So we map category & sub_category columns with respect to the product_name column\n",
        "\n",
        "category_map = {\n",
        "    # cayrgory = footwear\n",
        "  \"shoe\": (\"footwear\", \"shoes\"),\"sneaker\": (\"footwear\", \"shoes\"),\"running\": (\"footwear\", \"running shoes\"),\"trainer\": (\"footwear\", \"trainers\"),\"cleat\": (\"footwear\", \"cleats\"),\n",
        "    \"slipper\": (\"footwear\", \"slippers\"),\"flip flop\": (\"footwear\", \"flip flops\"),\"jordan\": (\"footwear\", \"basketball shoes\"),\"retro\": (\"footwear\", \"shoes\"),\n",
        "    \"phantom\": (\"footwear\", \"cleats\"),\"venom\": (\"footwear\", \"cleats\"),\"mercurial\": (\"footwear\", \"soccer shoes\"),\"superfly\": (\"footwear\", \"soccer shoes\"),\n",
        "    \"tf\": (\"footwear\", \"turf soccer shoes\"),\"air max\": (\"footwear\", \"sneakers\"),\"p-6000\": (\"footwear\", \"running shoes\"),\"sandal\": (\"footwear\", \"sandals\"),\n",
        "    \"slide\": (\"footwear\", \"slides\"),\"adilette\": (\"footwear\", \"slides\"),\"flipflop\": (\"footwear\", \"flip flops\"),\"sb\": (\"footwear\", \"skate shoes\"),\"skate\": (\"footwear\", \"skate shoes\"),\n",
        "    \"chron\": (\"footwear\", \"skate shoes\"),\"kd\": (\"footwear\", \"basketball shoes\"),\"kyrie\": (\"footwear\", \"basketball shoes\"),\"iconclash\": (\"footwear\", \"running shoes\"),\n",
        "    \"daybreak\": (\"footwear\", \"sneakers\"),\"blazer\": (\"footwear\", \"sneakers\"),\"prelove\": (\"footwear\", \"sneakers\"),\"pegasus\": (\"footwear\", \"running shoes\"),\n",
        "    \"vaporfly\": (\"footwear\", \"running shoes\"),\"zoomx\": (\"footwear\", \"running shoes\"),\"slipon\": (\"footwear\", \"slip-ons\"),\"airforce\": (\"footwear\", \"sneakers\"),\n",
        "    \"airmax\": (\"footwear\", \"sneakers\"),\"metcon\": (\"footwear\", \"training shoes\"),\"court\": (\"footwear\", \"tennis shoes\"),\"pg\": (\"footwear\", \"basketball shoes\"),\n",
        "    \"m2k\": (\"footwear\", \"sneakers\"),\"winflo\": (\"footwear\", \"running shoes\"),\"vomero\": (\"footwear\", \"running shoes\"),\"vapormax\": (\"footwear\", \"lifestyle sneakers\"),\n",
        "    \"flip-flop\": (\"footwear\", \"flip flops\"),\"flip-flops\": (\"footwear\", \"flip flops\"),\"slip-on\": (\"footwear\", \"slip-ons\"), \"slip-ons\": (\"footwear\", \"slip-ons\"),\n",
        "    \"odyssey react\": (\"footwear\", \"running shoes\"),\"legend react\": (\"footwear\", \"running shoes\"),\"pre-love\": (\"footwear\", \"sneakers\"),\"air force\": (\"footwear\", \"sneakers\"),\n",
        "    \"drop-type\": (\"footwear\", \"running shoes\"),\"zoom rival fly\": (\"footwear\", \"running shoes\"),\"mx-720-818\": (\"footwear\", \"running shoes\"),\"tanjun\": (\"footwear\", \"running shoes\"),\n",
        "    \"superstar\": (\"footwear\", \"sneakers\"),\"slip on\": (\"footwear\", \"slip-ons\"),\"lebron soldier\": (\"footwear\", \"basketball shoes\"),\"react element\": (\"footwear\", \"running shoes\"),\n",
        "    \"free rn\": (\"footwear\", \"running shoes\"),\"zoom fly\": (\"footwear\", \"running shoes\"),\"zoom rise\": (\"footwear\", \"running shoes\"),\"tiempo legend\": (\"footwear\", \"soccer shoes\"),\n",
        "    \"flex rn\": (\"footwear\", \"running shoes\"),\"air zoom structure\": (\"footwear\", \"running shoes\"),\"sfb gen 2\": (\"footwear\", \"boots\"),\"air huarache\": (\"footwear\", \"sneakers\"),\n",
        "    \"wildhorse\": (\"footwear\", \"running shoes\"),\"benassi\": (\"footwear\", \"slides\"),\"terra kiger\": (\"footwear\", \"running shoes\"),\"classic cortez\": (\"footwear\", \"sneakers\"),\n",
        "    \"renew run\": (\"footwear\", \"running shoes\"),\"free tr\": (\"footwear\", \"training shoes\"),\"lebron\": (\"footwear\", \"basketball shoes\"),\"mowabb\": (\"footwear\", \"sneakers\"),\n",
        "    \"revolution\": (\"footwear\", \"running shoes\"),\"precision\": (\"footwear\", \"basketball shoes\"),\"shox\": (\"footwear\", \"running shoes\"),\"potential\": (\"footwear\", \"basketball shoes\"),\n",
        "    \"epic react\": (\"footwear\", \"running shoes\"), \"react city\": (\"footwear\", \"running shoes\"),\"kawa\": (\"footwear\", \"slides\"),\"joyride run\": (\"footwear\", \"running shoes\"),\n",
        "    \"joyride optik\": (\"footwear\", \"running shoes\"),\"flex contact\": (\"footwear\", \"running shoes\"),\"football\": (\"footwear\", \"Football Shoes\"),\"predator\": (\"footwear\", \"Football Shoes\"),\n",
        "    \"vandalised\": (\"footwear\", \"Casual Shoes\"),\"canyon\": (\"footwear\", \"Casual Shoes\"),\"react\": (\"footwear\", \"Running Shoes\"),\"acg\": (\"footwear\", \"Outdoor Shoes\"),\n",
        "    \"flex\": (\"footwear\", \"Training Shoes\"),\"signal\": (\"footwear\", \"Running Shoes\"),\"joyride\": (\"footwear\", \"Running Shoes\"),\"cortez\": (\"footwear\", \"Casual Shoes\"),\n",
        "    \"hawkins\": (\"footwear\", \"Casual Shoes\"),\"nemeziz\": (\"footwear\", \"Football Shoes\"),\"indoor\": (\"footwear\", \"Indoor Shoes\"),\"outdoor\": (\"footwear\", \"Outdoor Shoes\"),\n",
        "    \"trail\": (\"footwear\", \"Outdoor Shoes\"),\"superrep\": (\"footwear\", \"Training Shoes\"),\"zoom\": (\"footwear\", \"Running Shoes\"),\"tr\": (\"footwear\", \"Training Shoes\"),\n",
        "    \"renew\": (\"footwear\", \"Running Shoes\"),\"ghost\": (\"footwear\", \"Running Shoes\"),\"racer\": (\"footwear\", \"Running Shoes\"),\"alphadunk\": (\"footwear\", \"Basketball Shoes\"),\n",
        "    \"monarch\": (\"footwear\", \"Walking Shoes\"),\"af-1\": (\"footwear\", \"Casual Shoes\"),\"bella\": (\"footwear\", \"Casual Shoes\"), \"huarache\": (\"footwear\", \"Lifestyle Shoes\"),\n",
        "    \"solarsoft\": (\"footwear\", \"Training Shoes\"),\"exp-x14\": (\"footwear\", \"Running Shoes\"),\"fly.by\": (\"footwear\", \"Basketball Shoes\"),\"xarr\": (\"footwear\", \"Training Shoes\"),\n",
        "    \"skarn\": (\"footwear\", \"Casual Shoes\"),\"tailwind\": (\"footwear\", \"Running Shoes\"), \"air dsvm\": (\"footwear\", \"Running Shoes\"),\n",
        "    # category = accessories\n",
        "    \"sock\": (\"accessories\", \"socks\"), \"cap\": (\"accessories\", \"cap\"),\"hat\": (\"accessories\", \"cap\"),\"bag\": (\"accessories\", \"bag\"),\"backpack\": (\"accessories\", \"bag\"),\n",
        "    \"watch\": (\"accessories\", \"watch\")\n",
        "    }\n",
        "\n",
        "def categorize_product(name):\n",
        "    name = str(name).lower()\n",
        "    for keyword, (cat, subcat) in category_map.items():\n",
        "        if keyword in name:\n",
        "            return cat, subcat\n",
        "    return \"Other\", \"Other\"  # fallback if no keyword found\n",
        "\n",
        "adidas_data[[\"category\", \"sub_category\"]] = adidas_data[\"product_name\"].apply(lambda x: pd.Series(categorize_product(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rUxhLGRoA2ck",
        "outputId": "07bb3586-c647-469f-eb2d-02315e5c9503"
      },
      "outputs": [],
      "source": [
        "# arranging the columns to be the same in all datasets\n",
        "\n",
        "adidas_data = adidas_data[[\"product_name\", \"brand\",\"category\", \"sub_category\", \"description\"]]\n",
        "print(adidas_data.columns)\n",
        "data_details(adidas_data , n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LZkSA8xZA2ck",
        "outputId": "775aac1a-2b53-4efc-ae25-39fcd93b5197"
      },
      "outputs": [],
      "source": [
        "# DataFrame 3\n",
        "\n",
        "amazon_data =dataframes[\"df3\"]\n",
        "data_details(amazon_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "EYtC3iulA2ck",
        "outputId": "05630f5b-6085-4e37-d399-09b796ee7b13"
      },
      "outputs": [],
      "source": [
        "# there is no null values in the prefered dataset features\n",
        "#  Amazon dataset don't contain brand , we note the first name in the product_name is the brand\n",
        "# So creating a function to map the brand column with respect to product_name column\n",
        "\n",
        "def map_brand(name):\n",
        "    return name.split()[0]\n",
        "\n",
        "# Apply function\n",
        "amazon_data['brand'] =amazon_data['product_name'].apply(map_brand)\n",
        "\n",
        "#  Amazon dataset don't contain sub_category , we note the values in category colums are diveded by |\n",
        "# So creating it by map sub_category column with respect to category column by extracting the most specific level(last part)\n",
        "\n",
        "amazon_data['sub_category'] = amazon_data['category'].apply(lambda x: x.split('|')[-1])\n",
        "\n",
        "amazon_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7D3TXuhcA2cl",
        "outputId": "18525135-4451-4ecf-f43d-87178c270fe4"
      },
      "outputs": [],
      "source": [
        "# arranging the columns to be the same in all datasets\n",
        "\n",
        "amazon_data = amazon_data[[\"product_name\", \"brand\",\"category\", \"sub_category\", \"about_product\"]]\n",
        "amazon_data= amazon_data.rename(columns={\"about_product\": \"description\"})\n",
        "print(amazon_data.columns)\n",
        "data_details(amazon_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PATmymqaA2cl",
        "outputId": "dda84e50-8026-4b9e-871b-8ba43d6b05e7"
      },
      "outputs": [],
      "source": [
        "# DataFrame 4\n",
        "\n",
        "flipkart_data=dataframes[\"df4\"]\n",
        "data_details(flipkart_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "b5tBxkX_A2cl",
        "outputId": "b851bf79-3c02-4241-d617-9191b1b9ba28"
      },
      "outputs": [],
      "source": [
        "# there are nulls in description an brand columns\n",
        "# clearing \"discription\" rows with missed values\n",
        "\n",
        "flipkart_data=flipkart_data.dropna(subset=[\"description\"])\n",
        "flipkart_data =flipkart_data[flipkart_data['description'] != 'No description']\n",
        "flipkart_data.isnull().sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWY5p-9BA2cl",
        "outputId": "716ac2bc-ab85-4eb5-a4ff-f7561c8f8b8d"
      },
      "outputs": [],
      "source": [
        "# display the most common brand to fill the missing value\n",
        "\n",
        "most_common = flipkart_data['brand'].mode()[0]\n",
        "print(most_common)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5UmcXwIA2cl",
        "outputId": "5bc6d08f-979e-4c62-9dd7-37964c790fb2"
      },
      "outputs": [],
      "source": [
        "# filling the missed value of brand By common brand in our dataset \"REEB\"\n",
        "\n",
        "flipkart_data['brand'].fillna(\"REEB\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "P-tj-JiFA2cm",
        "outputId": "44470164-036f-4c91-f0f7-13e78f30c6cb"
      },
      "outputs": [],
      "source": [
        "# arranging the columns to be the same in all datasets\n",
        "\n",
        "flipkart_data = flipkart_data[[\"title\", \"brand\",\"category\", \"sub_category\", \"description\"]]\n",
        "flipkart_data= flipkart_data.rename(columns={\"title\": \"product_name\"})\n",
        "print(flipkart_data.columns)\n",
        "flipkart_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k8oM5YuBA2cm",
        "outputId": "a0067417-378c-4f40-b33b-3af0c45bdf09"
      },
      "outputs": [],
      "source": [
        "# DataFrame 5\n",
        "\n",
        "adidas2_data =dataframes[\"df5\"]\n",
        "data_details(adidas2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sYUTbsoRA2cm",
        "outputId": "05cc6be1-87d4-4ce3-8a6a-478899da654c"
      },
      "outputs": [],
      "source": [
        "# adidas2 dataset not have null values\n",
        "#  Noting the breadcrumbs colums contains sub_category\n",
        "\n",
        "adidas2_data = adidas2_data[[\"name\", \"brand\",\"category\", \"breadcrumbs\", \"description\"]]\n",
        "adidas2_data= adidas2_data.rename(columns={\"name\": \"product_name\" , \"breadcrumbs\":\"sub_category\"})\n",
        "print(adidas2_data.columns)\n",
        "data_details(adidas2_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hIAUGVZ5A2cm",
        "outputId": "236c7887-81ee-42db-9029-8f48097cf2a5"
      },
      "outputs": [],
      "source": [
        "# DataFrame 6\n",
        "\n",
        "elec_data = dataframes[\"df6\"]\n",
        "data_details(elec_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaSlKlGhA2cm",
        "outputId": "e3767155-93f7-476f-ed3f-f63a48e806a1"
      },
      "outputs": [],
      "source": [
        "# The elec_data dataSet is clear\n",
        "\n",
        "elec_data= elec_data.rename(columns={\"Product_name\": \"product_name\"})\n",
        "print(elec_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55OQrwwjA2cn",
        "outputId": "5bbb01e6-0424-47df-e37c-72a9957c1053"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "StV0OpfxA2cn",
        "outputId": "6f3be67b-6ac3-4edd-8df9-d8b1cc05c039"
      },
      "outputs": [],
      "source": [
        "# DataFrame 7\n",
        "\n",
        "Bigbasket2 = dataframes[\"df7\"]\n",
        "data_details(Bigbasket2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lRgB0Lp2A2cn",
        "outputId": "5d692e06-0ece-44a0-9efc-06632dcf0f5e"
      },
      "outputs": [],
      "source": [
        "Bigbasket2=Bigbasket2[[\"SKU Name\",\"Brand\",\"Category\",\"Sub-Category\",\"About the Product\"]]\n",
        "data_details(Bigbasket2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "izbE_retA2cn",
        "outputId": "4ba5aadc-13c0-4ff0-ce10-1a3d438435fe"
      },
      "outputs": [],
      "source": [
        "# rename the column and clean the row with null or no description\n",
        "\n",
        "Bigbasket2=Bigbasket2.rename(columns={\"SKU Name\": \"product_name\" , \"Brand\":\"brand\",\"Category\":\"category\", \"Sub-Category\":\"sub_category\" ,\"About the Product\":\"description\"})\n",
        "Bigbasket2=Bigbasket2.dropna(subset=[\"description\"])\n",
        "Bigbasket2 =Bigbasket2[Bigbasket2['description'] != 'No description']\n",
        "Bigbasket2.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A34J8m3GA2co"
      },
      "outputs": [],
      "source": [
        "# the brand column has only 3 null values\n",
        "\n",
        "Bigbasket2=Bigbasket2.dropna(subset=[\"brand\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "29tc1uGRA2co",
        "outputId": "0e750a7e-a929-41da-a9d4-6203b5b1eec4"
      },
      "outputs": [],
      "source": [
        "data_details(Bigbasket2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2JZ6G5HA2co"
      },
      "outputs": [],
      "source": [
        "# Combining all datasets\n",
        "\n",
        "Final_data = pd.concat([basket_data, adidas_data, amazon_data ,adidas2_data , flipkart_data , elec_data  ,Bigbasket2], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gEwf8ycVA2co",
        "outputId": "41584da0-504f-4aaf-9183-6a6fe9cda581"
      },
      "outputs": [],
      "source": [
        "data_details(Final_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV0wa7cdFId2",
        "outputId": "61b23d76-b82d-4fbf-d287-6fe88c36bcb8"
      },
      "outputs": [],
      "source": [
        "preproc = TextGANPreprocessor()\n",
        "\n",
        "# Run full preprocessing and split\n",
        "data_splits = preprocess_and_split_data(Final_data, preproc, max_len_target=200)\n",
        "\n",
        "# Access ready DataLoaders\n",
        "# Using a small batch size for demonstration\n",
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(data_splits[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(data_splits[\"val\"], batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(data_splits[\"test\"], batch_size=BATCH_SIZE)\n",
        "\n",
        "vocab_size = data_splits[\"vocab_size\"]\n",
        "max_len = data_splits[\"max_len\"]\n",
        "\n",
        "print(f\"\\n--- Model Hyperparameters ---\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Vocab Size = {vocab_size}\")\n",
        "print(f\"Max Sequence Length = {max_len}\")\n",
        "print(f\"Batch Size = {BATCH_SIZE}\")\n",
        "print(f\"-----------------------------\")\n",
        "\n",
        "\n",
        "# Train Word2Vec & Build Embedding Matrix\n",
        "EMBED_DIM = 128\n",
        "w2v_model = train_word2vec(\n",
        "        data_splits[\"clean_df\"],\n",
        "        vector_size=EMBED_DIM,\n",
        "        window=5,\n",
        "        min_count=1,\n",
        "        epochs=10\n",
        ")\n",
        "\n",
        "# Build embedding matrix \n",
        "print(\"Building embedding matrix...\")\n",
        "embedding_matrix = np.random.randn(vocab_size, EMBED_DIM)\n",
        "for word, idx in preproc.word2idx.items():\n",
        "    if word in w2v_model.wv:\n",
        "          embedding_matrix[idx] = w2v_model.wv[word]\n",
        "embedding_matrix = torch.tensor(embedding_matrix,dtype=torch.float)\n",
        "print(\"Embedding matrix built.\")\n",
        "\n",
        "# Initialize models\n",
        "HIDDEN_DIM = 256\n",
        "COND_DIM = 128\n",
        "\n",
        "print(\"Initializing models...\")\n",
        "generator = Generator(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        cond_dim=COND_DIM,\n",
        "        max_len=max_len,\n",
        "        embedding_matrix=embedding_matrix\n",
        ").to(device)\n",
        "\n",
        "discriminator = Discriminator(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        embedding_matrix=embedding_matrix\n",
        ").to(device)\n",
        "print(\"Models initialized.\")\n",
        "\n",
        "# Train GAN\n",
        "print(\"\\n--- Starting GAN Training ---\")\n",
        "train_gan(\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        dataloader=train_loader,\n",
        "        num_epochs=20, \n",
        "        vocab_size=vocab_size,\n",
        "        cond_dim=COND_DIM,\n",
        "        preprocessor=preproc\n",
        ")\n",
        "print(\"--- GAN Training Complete ---\")\n",
        "\n",
        "# Generate one description\n",
        "print(\"\\nGenerating a sample description from the validation set...\")\n",
        "\n",
        "# Load the best generator\n",
        "try:\n",
        "        generator.load_state_dict(torch.load(\"best_generator.pth\", map_location=device))\n",
        "        print(\"Loaded best generator weights.\")\n",
        "except FileNotFoundError:\n",
        "        print(\"No saved generator found. Using last epoch's weights.\")\n",
        "\n",
        "sample_input, _ = next(iter(val_loader))\n",
        "sample_input_single = sample_input[0].unsqueeze(0).to(device)\n",
        "\n",
        "# CORRECTED FUNCTION CALL \n",
        "generated_description = generate_description(\n",
        "        generator=generator,\n",
        "        preprocessor=preproc,\n",
        "        input_seq=sample_input_single,\n",
        "        cond_dim=COND_DIM,\n",
        "        device=device,\n",
        "        max_len=max_len # Use max_len for output truncation\n",
        ")\n",
        "\n",
        "original_input_text = preproc.decode_text(sample_input_single[0].tolist())\n",
        "print(f\"\\n Source Input Text:\\n{original_input_text}\")\n",
        "print(f\"\\n Generated Description:\\n{generated_description}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "depi_projects",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
